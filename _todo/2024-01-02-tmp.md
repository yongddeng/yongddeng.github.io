---
layout: default
title: "402. operating system"
tags: cs400
use_math: true
---


# Operating System
---
A computer is ultimately a set of interconnected electronic components. The operating system makes it usable by hiding complexity and offering a programmable interface. Though often taken for granted, the OS is less than a century old yet underpins nearly all computing devices today. We inevitably encounter it when moving from high-level code down to low-level hardware instructions

<!-- https://pravin-hub-rgb.github.io/BCA/resources/sem2/operating_sys/index.html -->
<!-- https://www.jmeiners.com/lc3-vm/#:lc3.c -->
<!-- https://www.youtube.com/watch?v=ioJkA7Mw2-U -->
<!-- https://www.youtube.com/watch?v=xFMXIgvlgcY -->
<!-- https://youtu.be/eP_P4KOjwhs?si=gOPQIxLH6cQMk8vq -->

<!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/26QPDBe-NB8?si=J8FBjnS8tE8PHY5k" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe> -->

<!-- round robin, fifo, ... -->

<!-- If data is large or its size varies, we use heap, and in stack, we just maintain a reference (i.e. pointer) to the value.... The *malloc* function in C internally uses *mmap* to free up a dedicated space and reclaim the OS for reusability. *free* does .... By using linked list, we do not need large amounts of contiguous memory, although the this data structure leads to decreased probabilities of cache hits. If our aim is to maintain compactness in our list, what we need is an array list (e.g. an array wrapped in the C struct with relevant metadata) -->


## I
---

<!-- https://pravin-hub-rgb.github.io/BCA/resources/sem2/images/typesc.svg -->

<!-- 
### **1.4. OS Virtualisation**
<p style="margin-bottom: 12px;"> </p>


- https://insights.sei.cmu.edu/blog/virtualization-via-containers/

- https://blog.bytebytego.com/p/what-are-the-differences-between

- https://stackoverflow.com/questions/71955661/what-is-the-difference-between-containers-and-process-vms-not-system-vms  

Virtualization abstracts hardware resources, enabling multiple virtual systems to run on a single physical machine as though each were operating independently. It improves resource utilization, isolation, and scalability, forming the backbone of modern computing environments. Hardware virtualization, achieved through hypervisors, is a key approach. Type 1 hypervisors like VMware ESXi run directly on hardware for high performance, while Type 2 hypervisors like VirtualBox operate atop an existing OS, providing ease of use at the cost of efficiency.

OS-level virtualization, exemplified by containers like Docker, isolates applications within lightweight environments that share the host OS kernel. Containers are faster to deploy and consume fewer resources than virtual machines, making them ideal for microservices and cloud-native applications. However, their reliance on a shared kernel requires robust isolation mechanisms to maintain security.

- <div style="position: relative; display: inline-block;"> <img src="https://insights.sei.cmu.edu/media/images/fourthpost_firstgraphic_092520.max-1280x720.format-webp.webp" width="500"> <a href="https://insights.sei.cmu.edu/blog/virtualization-via-containers/" target="_blank" style="position: absolute; top: 0px; left: 4px; font-size: 12px;">[src]</a> </div> -->

<!-- Containersiation (i.e. OS-level virtualisation using a conainer) is ... || https://blog.bytebytego.com/p/what-are-the-differences-between -->


## II
---

### **2.1. Unix**

<!-- 
Although Bell Labs owned Unix, its parent company AT&T was a regulated monopoly under the 1934 Communications Act and a 1956 consent decree. This prohibited AT&T from commercialising computing products. As a result, Unix was licensed freely to universities and research institutions. The influence of Unix deepened with the emergence of the [Berkeley Software Distribution]() (BSD) in the late 1970s. Initiated by [Bill Joy]() and the [Computer Systems Research Group]() (CSRG) at the University of California, Berkeley, BSD originally extended AT&T Unix with tools and utilities, but matured enough by the late 1980s to be a distinct OS. 

Under DARPA funding, BSD introduced many features foundational to modern networked computing. It includes the first implementation of the [TCP/IP]() protocol stack which made BSD a reference platform for early [Internet]() research and deployment. Permissive licence of BSD, combined with its technical maturity, also made it highly attractive to commercial vendors. Variants such as SunOS, DEC Ultrix, and NeXTSTEP built directly on BSD foundations, as did later research and production systems in academia. Its legacy lives on in its open-source descendants: FreeBSD, NetBSD, OpenBSD, and DragonFly BSD. 

These projects have since formed the basis for numerous modern systems, including Apple’s [Darwin](), which underpins macOS and iOS, and parts of Microsoft Windows’ networking stack. BSD-derived code has also appeared in consumer devices ranging from routers to games consoles such as [PS5](). Nevertheless, vendor-specific extensions led to fragmentation across the Unix ecosystem. To mitigate this, the [IEEE]() introduced the [portable operating system interface]() (POSIX) standard in the late 1980s. POSIX defined a consistent set of APIs and command-line behaviours, allowing software to be portable across Unix-like systems. While not eliminating all incompatibilities, POSIX significantly unified the Unix landscape. -->



<!-- 
<p style="margin-bottom: 12px;"> </p>

OS Implementations have been evolving ... || Unix, first developed at AT&T’s Bell Labs in 1969 and released in 1971, revolutionized operating system design with its emphasis on portability, modularity, and simplicity. Unlike earlier systems tied to specific hardware, Unix was written in the C programming language, allowing it to be easily ported across different architectures. This innovation made Unix one of the first truly versatile operating systems. Its hierarchical file system introduced a logical and consistent method for organizing files and directories, while features like pipes and redirection enabled powerful command-line utilities to be combined for complex tasks. These characteristics set the foundation for Unix’s widespread adoption in academia, research, and enterprise environments.

Unix introduced a philosophy of building small, modular tools that perform specific tasks well, which could be combined to achieve greater functionality. This approach not only made Unix extensible but also encouraged innovation in software development. Key principles, such as treating everything as a file (including devices), further simplified interactions between hardware and software. Over the decades, Unix evolved into various commercial and open-source derivatives, including BSD, AIX, and HP-UX, which retained its foundational principles while adding enhancements tailored to specific use cases.

Today, Unix serves as the backbone of many modern operating systems, influencing everything from Linux to macOS. Its legacy lives on in the standardized POSIX interface, ensuring compatibility across Unix-like systems. Unix’s design principles remain relevant, providing a model for creating robust, efficient, and scalable software systems that have stood the test of time.

<!-- [Unix](), first released in 1971, revolutionised OS design with portability, hierarchical file systems, and modularity. -->

### **3.2. Linux**
<p style="margin-bottom: 12px;"> </p>

Linux, introduced by Linus Torvalds in 1991, is an open-source operating system inspired by Unix. As a kernel, Linux became the foundation for a wide variety of distributions (distros), each tailored to different use cases. Unlike proprietary systems, Linux’s open-source nature encourages collaboration and innovation, allowing developers worldwide to contribute improvements. It inherits Unix’s modularity and hierarchical file system, making it familiar to those with Unix experience.

Linux is highly adaptable, running on devices ranging from personal computers to servers, smartphones, and embedded systems. Its flexibility, combined with a strong focus on security and stability, has made it a favorite for developers, system administrators, and enterprises. Linux distributions often bundle the Linux kernel with system utilities and applications to form a complete operating system. Popular distros include Ubuntu, Fedora, Debian, and CentOS, each catering to specific audiences, from beginners to advanced users and enterprise environments.

Despite being a descendant of Unix, Linux stands out due to its emphasis on community-driven development. The GPL (General Public License) ensures that Linux remains free to use, modify, and distribute. This has led to its widespread adoption in cloud computing, web hosting, and supercomputing. As a direct competitor to proprietary systems like Windows and macOS, Linux continues to grow, shaping the future of open-source computing.

<!-- [Linux]() and [macOS](), as the main alternatives to Windows, are all decendants of Unix, and so have common on their structure and conventions. || The prior is an open-source OS that has led to the development of  -->

<!-- [Disk operating system]() (DOS) also made a huge impact on personal computing. While DOS provided file systems like FAT12 and FAT16 which users could manage files on disk, directories, and applications through [command-line interfaces]() (CLI), it merely operated on basic hardware components and experienced limitations in memory access (i.e. 640KB), multitasking, and native support for [graphical user interfaces]() (GUI). Microsoft (through acquisition) rebranded it as MS-DOS in 1981, and later introduced a graphical interface with Windows 1.0 layered on top of MS-DOS in 1985. MS-DOS was compatible with IBM PCs in the 1980s, but shortly [Windows]() evolved into a standalone OS in the 1990s. -->

<!-- - <div style="position: relative; display: inline-block;"> <img src="../assets/blog/2024-01-02-linux_overview.png" width="400"> <a href="https://www.instagram.com/p/DExccqLTHZo/" target="_blank" style="position: absolute; top: 0px; left: 4px; font-size: 12px;">[src]</a> </div> -->


### **3.3. Filesystem Hierarchy Standard**

The Filesystem Hierarchy Standard (FHS) is a key convention defining the directory structure and layout of Unix-like operating systems. It provides guidelines for organizing files and directories to ensure consistency and predictability across systems. For instance, the root directory (”/”) serves as the base, with standard subdirectories like /bin for essential binaries, /etc for configuration files, and /var for variable data like logs and caches. This standardization simplifies system administration and software development by creating a predictable environment for locating files.

FHS ensures that critical system components are logically separated. For example, /usr houses user-installed software, while /home contains personal files for individual users. Temporary files are stored in /tmp, ensuring they do not interfere with other parts of the system. This separation enhances both security and maintainability, making it easier to back up user data, update system files, or troubleshoot issues.

While not all Unix-like systems strictly adhere to FHS, most Linux distributions follow it closely, ensuring interoperability and simplifying the learning curve for users and administrators. By providing a clear structure, FHS contributes to the reliability and organization of Unix-like systems, reinforcing their reputation for robustness and ease of use.

### **3.4. Linux Distro**
<p style="margin-bottom: 12px;"> </p>

Linux distributions, commonly referred to as distros, are complete operating systems built around the Linux kernel. They bundle system utilities, libraries, and applications to provide a cohesive user experience. Each distribution is tailored to specific user needs, whether it’s for general use, development, enterprise systems, or specialized environments. Popular examples include Ubuntu, known for its user-friendliness and beginner appeal, and Debian, a stable and community-driven distro that serves as the foundation for many others, including Ubuntu itself. Fedora offers cutting-edge software and is often used by developers, while CentOS provides a stable environment for servers.

Enterprise distributions like Red Hat Enterprise Linux (RHEL) and SUSE Linux Enterprise are designed for corporate environments, offering professional support and long-term stability. In contrast, lightweight distros like Arch Linux and Alpine Linux cater to advanced users and specific use cases, such as minimalistic systems or containerized environments. The diversity of Linux distros ensures that users can find a version suited to their technical proficiency and requirements.

The strength of Linux distributions lies in their customizability. Users can tailor the system to their preferences, choosing desktop environments (e.g., GNOME, KDE) or adding specific packages. The package management systems used by distros, such as APT for Debian-based systems or YUM/DNF for Red Hat-based ones, make software installation and updates seamless. This diversity and flexibility underscore the unique appeal of Linux in the computing world.



## III
---
### **2.5. IPC**
<p style="margin-bottom: 12px;"> </p>


### **2.1. Process Management**
<p style="margin-bottom: 12px;"> </p>

A process is an execution unit that requires hardware resources, such as CPU time and memory, to function. An OS handles processes by assigning resources and CPU time

	•	Process states (e.g., running, waiting, ready).
	•	Context switching.
	•	Interprocess communication (IPC).
	•	Deadlock handling.
	•	Process synchronization.

[Process Life Cycle]() introduces process states (e.g., New, Running, Waiting, Terminated) || [Scheduling Algorithms](): Explain scheduling types like Round-Robin, First-Come-First-Served, and Priority-based, emphasizing their impact on performance. || A [thread]() is the smallest unit of ...
 
- <div style="position: relative; display: inline-block; background-color: white;"> <img src="https://pravin-hub-rgb.github.io/BCA/resources/sem2/images/process2.svg" width="500"> <a href="https://pravin-hub-rgb.github.io/BCA/resources/sem2/operating_sys/unit2/index.html" target="_blank" style="position: absolute; bottom: -8px; right: 4px; font-size: 12px;">[src]</a> </div>


[Context switching]() refers to ...

- <div style="position: relative; display: inline-block; background-color: white;"> <img src="https://pravin-hub-rgb.github.io/BCA/resources/sem2/images/contextswitching.svg" width="500"> <a href="https://pravin-hub-rgb.github.io/BCA/resources/sem2/operating_sys/unit2/index.html" target="_blank" style="position: absolute; bottom: -8px; right: 4px; font-size: 12px;">[src]</a> </div>

### **2.2. Memory Management**
<p style="margin-bottom: 12px;"> </p>

The main responsiblities of modern OS includes... [Memory Management](): Explain how the OS allocates, protects, and releases memory across applications, handling both RAM and virtual memory. || [Paging and Segmentation](): Cunks of memory are called pages. Explain these memory management techniques, their differences, and roles in efficient memory use. || [Virtual Memory](): Describe its role in extending physical memory and how the OS handles page replacement policies (e.g., Least Recently Used).

<!-- Memory Allocation:
	•	When a program requests memory (e.g., using malloc in C), the runtime library typically makes system calls like brk or mmap to ask the OS for memory.
	•	The OS allocates memory from available resources, keeping track of which parts of memory are free or in use.
	3.	Paging: If the system runs out of physical memory, the OS may use a swap file on disk to simulate more memory, moving data between RAM and disk as needed. -->


<!-- A quick recap from the undergraduate operating systems course: information is grouped into pages in memory, and sometimes, we need to transfer chunks of information from one page to another. This is another hassle we must contend with when transferring information from the CPU to the GPU or vice-versa. 

NVIDIA has a new unified memory feature that does automatic page-to-page transfers between the CPU and GPU for error-free GPU processing when the GPU occasionally runs out of memory. The authors use this feature to allocate paged memory for the optimizer states, which are then automatically evicted to CPU RAM when the GPU runs out of memory and paged back into GPU memory when the memory is needed in the optimizer update step.

An operating system (OS) facilitates the execution of machine code by loading the binary executable into memory, setting up the process environment, and delegating its execution to the CPU. It reads the executable’s format (e.g., ELF, PE, or Mach-O), allocates memory, and initializes resources like the stack and heap. The CPU fetches and executes the machine code instructions directly, while the OS manages system calls, resource allocation, and interruptions to ensure smooth execution. Although the OS doesn’t interpret machine code itself, it plays a critical role in organizing and controlling its execution.

- https://wandb.ai/sauravmaheshkar/QLoRA/reports/What-is-QLoRA---Vmlldzo2MTI2OTc5
- https://stackoverflow.com/questions/26891413/diff-between-logical-memory-and-physical-memory -->


### **2.3. File Management**
<p style="margin-bottom: 12px;"> </p>


<!-- - <div style="position: relative; display: inline-block; background-color: white;"> <img src="https://swcarpentry.github.io/shell-novice/fig/standard-filesystem-hierarchy.svg" width="500" height="300"> <a href="https://swcarpentry.github.io/shell-novice/reference.html" target="_blank" style="position: absolute; bottom: -8px; right: 4px; font-size: 12px;">[src]</a> </div> -->

A file is .... Note that a file is a passive storage entity for data, while a program is an active entity designed to manipulate data and perform actions, often using files as part of its operation....

[File System](): Structure and manage data storage on drives, handling file creation, access, and organization. || [File Allocation Methods](): Briefly mention methods like contiguous allocation and inode-based systems. || [Journaling](): Discuss how modern file systems like NTFS and ext4 log changes before making them permanent, enhancing data integrity.

### **2.4. Device Management**
<p style="margin-bottom: 12px;"> </p>

[Device Management and Drivers](): Role of drivers in enabling communication between OS and hardware devices. || [Plug and Play]() (PnP): Describe how modern OSs automatically detect and configure new devices. -->

<!-- - responsible for memory allocation (stack and data regions); devs should be able to magage mem in heap region (e.g. malloc) -->

<!-- Paging -->
<!-- User mode (vs kernel model) -->
<!-- File systems -->
<!-- System calls -->
<!-- Multitasking (multi-threading or multi-processing) -->

-->