<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
	<link rel="stylesheet" href="/assets/css/atom-one-light.css">
    
        <title>205. characteristic function</title>
		<link rel="stylesheet" type="text/css" href="/assets/css/002.css">
    
	<link rel="stylesheet" href="/assets/css/font-awesome.min.css">
	<link rel="shortcut icon" href="/assets/img/favicon.ico" type="image/x-icon">
	<link rel="icon" href="/assets/img/favicon.ico" type="image/x-icon">
	<script src="/assets/js/highlight.pack.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>
	
		<!--http://benlansdell.github.io/computing/mathjax/-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    extensions: [
      "tex2jax.js",
      "MathMenu.js",
      "MathZoom.js",
      "AssistiveMML.js",
      "a11y/accessibility-menu.js"
    ],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    jax: ["input/TeX", "output/CommonHTML"],
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
</script>
<!-- You may add the following into MathJax.Hub.Config:
CommonHTML: {
    scale: 85
}-->
<script type="text/javascript" id="MathJax-script" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
	
</head>
<body>
	<div class="wrapper">
		<div class="default_title">
			<img src="/assets/img/mycomputer.png" />
			
				<h1>Hikikomori</h1>
			
		</div>
		<ul class="topbar">
	<a href="/me"><li><u>A</u>bout</li></a>
	<a href="https://www.linkedin.com/in/sung-kim-28b78a7b/" target="_blank"><li><u>L</u>inkedIn</li></a>
	<!-- 
		<a href="http://soundcloud.com/h01000110" target="_blank"><li><u>S</u>oundcloud</li></a>
	-->
</ul>
		<div class="tag_list">
			<ul id="tag-list">
				<li><a href="/" ><img src="/assets/img/disk.png" />(C:)</a>
			<ul>
				
				
				<li><a href="/tag/cs300/" title="cs300"><img src="/assets/img/folder.ico" />cs300</a></li>
				
				<li><a href="/tag/cs400/" title="cs400"><img src="/assets/img/folder.ico" />cs400</a></li>
				
				<li><a href="/tag/math200/" title="math200"><img src="/assets/img/folder.ico" />math200</a></li>
				
				<li><a href="/tag/math500/" title="math500"><img src="/assets/img/folder.ico" />math500</a></li>
				
			</ul>
				</li>
			</ul>
		</div>
		<div class="post_list">
			
				<ul>
					
					<li><a href="/20240201/functional-analysis" title="501. functional analysis"><img src="/assets/img/file.ico" title="501. functional analysis" />501. functional analysis</a></li>
					
					<li><a href="/20240105/parallel-computing" title="Parallel Computing"><img src="/assets/img/file.ico" title="Parallel Computing" />Parallel Computing</a></li>
					
					<li><a href="/20240104/networking" title="Networking"><img src="/assets/img/file.ico" title="Networking" />Networking</a></li>
					
					<li><a href="/20240103/virtualisation" title="Virtualisation"><img src="/assets/img/file.ico" title="Virtualisation" />Virtualisation</a></li>
					
					<li><a href="/20240102/tmp" title="402. operating system"><img src="/assets/img/file.ico" title="402. operating system" />402. operating system</a></li>
					
					<li><a href="/20240102/operating-system" title="402. operating system"><img src="/assets/img/file.ico" title="402. operating system" />402. operating system</a></li>
					
					<li><a href="/20240101/computer" title="401. dl compilation"><img src="/assets/img/file.ico" title="401. dl compilation" />401. dl compilation</a></li>
					
					<li><a href="/20230101/ml-paper" title="301. ml literature"><img src="/assets/img/file.ico" title="301. ml literature" />301. ml literature</a></li>
					
					<li><a href="/20220112/markov-chain" title="212. markov chain"><img src="/assets/img/file.ico" title="212. markov chain" />212. markov chain</a></li>
					
					<li><a href="/20220111/martingale" title="211. martingale"><img src="/assets/img/file.ico" title="211. martingale" />211. martingale</a></li>
					
					<li><a href="/20220110/stochastic-process" title="210. stochastic process"><img src="/assets/img/file.ico" title="210. stochastic process" />210. stochastic process</a></li>
					
					<li><a href="/20220109/laws-of-the-iterated-logarithm" title="209. law of the iterated logarithm"><img src="/assets/img/file.ico" title="209. law of the iterated logarithm" />209. law of the iterated logarithm</a></li>
					
					<li><a href="/20220108/central-limit-theorem" title="208. central limit theorem"><img src="/assets/img/file.ico" title="208. central limit theorem" />208. central limit theorem</a></li>
					
					<li><a href="/20220107/laws-of-large-number" title="207. law of large number"><img src="/assets/img/file.ico" title="207. law of large number" />207. law of large number</a></li>
					
					<li><a href="/20220106/modes-of-convergence" title="206. mode of convergence"><img src="/assets/img/file.ico" title="206. mode of convergence" />206. mode of convergence</a></li>
					
					<li><a href="/20220105/characteristic-function" title="205. characteristic function"><img src="/assets/img/file.ico" title="205. characteristic function" />205. characteristic function</a></li>
					
					<li><a href="/20220104/inequality" title="204. inequality"><img src="/assets/img/file.ico" title="204. inequality" />204. inequality</a></li>
					
					<li><a href="/20220103/expectation" title="203. expectated value"><img src="/assets/img/file.ico" title="203. expectated value" />203. expectated value</a></li>
					
					<li><a href="/20220102/random-variable" title="202. random variable"><img src="/assets/img/file.ico" title="202. random variable" />202. random variable</a></li>
					
					<li><a href="/20220101/probability" title="201. probability theory"><img src="/assets/img/file.ico" title="201. probability theory" />201. probability theory</a></li>
					
				</ul>
			
		</div>
		<div class="post_total">
			
				<div class="left">20 object(s)</div>
			
			<div class="right">&nbsp;</div>
		</div>
	</div>
	
        <div class="content">
			<div class="post_title">
				<img src="/assets/img/file.png" />
				<h1>205. characteristic function</h1>
				<a href="/"><div class="btn"><span class="fa fa-times"></span></div></a>
				<div class="btn btn_max"><span class="fa fa-window-maximize"></span></div>
				<div class="btn"><span class="fa fa-window-minimize"></span></div>
			</div>
			<ul class="topbar">
				<li>January 5, 2022</li>
			</ul>
			<div class="post_content">
        		<h1 id="characteristic-function">Characteristic Function</h1>
<hr />
<p>Let $\operatorname{FT}$ denotes the Fourier transform and $(f \ast g)(x) = \int_{\mathbb{R}} f(\tau)g(x-\tau)\,\mathrm{d}\tau$ be the convolution operator. The <a href="https://en.wikipedia.org/wiki/Convolution_theorem">convolution theorem</a> yields $\operatorname{FT} \lbrace f \ast g \rbrace = \operatorname{FT} \lbrace f \rbrace \cdot \operatorname{FT} \lbrace g \rbrace$. One may view a characteristic function $\varphi$ as the probabilistic version of $\operatorname{FT}$ that is applied to a density function $f_{Z}(z) = \int_{x \in X} f_{X}(x) f_{Y}(z-x)\,\mathrm{d}x$ of some random variable $Z = X+Y$.</p>

<h2 id="i">I</h2>
<hr />
<p>The <a href="https://en.wikipedia.org/wiki/Characteristic_function_(probability_theory)">characteristic function</a> (cf) $\varphi_X: \mathbb{R} \to \mathbb{C}$ of a random variable $X:\Omega\to\mathbb{R}$ is given by $\varphi_X(t) = \operatorname{E}e^{itX}$, where $i$ is the imaginary unit and $t \in \mathbb{R}$ is the argument. If $X$ admits $f_X$, then the definition $\varphi_X(t) = \int_{\mathbb{R}} e^{itx}f_X(x)\,\mathrm{d}x$ is equal to the <a href="https://en.wikipedia.org/wiki/Fourier_transform">Fourier transform</a> of $f_X$ with sign reversal in the exponent <strong>(#1)</strong>. If $Y = aX + b$, then $\varphi_Y(t)= e^{itb}\varphi_{X}(at)$. Furthermore, given a sequence $(X_n)_{n\in\mathbb{N}}$ of ind. random variables, if $S_n = \Sigma_{k=1}^{n} a_k X_k$, then $\varphi_{S_n}(t) = \prod_{k=1}^{n} \varphi_{X_k}(a_{k}t)$ due to the linearity $\operatorname{E}e^{it(aX+b)} = e^{itb}\operatorname{E}e^{itaX}$ and the multiplicativity $\operatorname{E}e^{it(X_1 + X_2 + \dots{+X_n})} = \prod_{k=1}^{n}\operatorname{E}e^{itX_k}$. Similar definitions and properties also hold for a random matrix $X_{m,n} \in \mathbb{R}^{m \times n}$.</p>

<p>Recall Jensen’s inequality ${\vert \varphi_X(t) \vert} = {\vert \operatorname{E}e^{itX} \vert} \leq \operatorname{E}{\vert e^{itX} \vert}$. Since ${\vert e^{itX} \vert} = {\vert \cos^2(tx)+i\sin^2(tx) \vert} = [\cos^2(tx)+\sin^2(tx)]^{1/2}=1$ and $P(\Omega) = 1$, we can tell that ${\vert \varphi_X(t) \vert} \leq 1$ and so the integral $\varphi_X$ is always well-defined on a space of finite measure. Also, $\varphi$ dose not vanish around zero as $\varphi_X(0) = 1$, In particular, the <a href="https://en.wikipedia.org/wiki/Riemann%E2%80%93Lebesgue_lemma">Riemann–Lebesgue lemma</a> states that the $\operatorname{FT}$ of an $L^1$-function vanishes at $\pm \infty$, and so $\lim_{t \to \pm \infty} {\vert \varphi_X(t) \vert} = 0$. To prove the uniform continuity of $\varphi_X$, we show that the magnitude $\vert \varphi_X(t+h)-\varphi_X(t) \vert$ is bounded by an arbitrarily $\varepsilon &gt; 0$ for any value $h &gt; 0$. It means that the estimate does not depend on the input argumnet $t\in\mathbb{R}$.</p>

<p>If the moment generating function $m_{X}$ exists for a random variable $X$, then $m_{X}(t) = \varphi_{X}(-it)$. Moreover, because $\varphi_X$ is <a href="">Hermitian</a>, i.e. $f_X = \operatorname{FT}^{-1}\lbrace \varphi_X \rbrace$ is real-valued, we can state $\varphi^{\ast}_X(t) = \varphi_{X}(-t)$, where $\ast$ denotes a complex conjugate. In particular, $\varphi_{X}$ of a symmetric random variable $X$ is a real-valued even function such that $\varphi_{X}(-t) = \varphi_{-X}(t)$. In terms of uniquness, $F_X=F_Y$ if and only if $\varphi_X = \varphi_Y$. The <a href="https://en.wikipedia.org/wiki/L%C3%A9vy%27s_continuity_theorem">Lévy’s continuity theorem</a> states that a bijection between $F_X$ and $\varphi_X$ is <a href="https://planetmath.org/sequentiallycontinuous">sequentially continuous</a>, and thus given any sequence $(X_n)_{n\in\mathbb{N}}$ <strong>(#2)</strong>, if $\varphi_{X_n} \to \varphi_X$ pointwisely as $n \to \infty$ (i.e. for all $t\in\mathbb{R}$), then one has $F_{X_n} \to F_X$ as $n\to\infty$.</p>

<h2 id="ii">II</h2>
<hr />
<p>Due to the continuity theorem, $\varphi_X$ massively eases proofs of theorems that is related to the distributional convergence of random variables and sums of random variables (i.e. the LLNs and the CLTs). Yet, we shall explore the features of Fourier transform in which a characteristic function advantages. The reciprocal relationship between the a Fourier transform and the <a href="https://en.wikipedia.org/wiki/Fourier_inversion_theorem">inverse Fourier transform</a> is a traditional example. Suppose $\varphi_X$ is integrable and $F_X$ is continuous. The inversion formula $f_X(x) = {1 \over 2\pi} \int_{\mathbb{R}} e^{-itx} \varphi_X(t)\,\mathrm{d}t$ provides the density function of $X$ beased on $\varphi_X$, or, equivalently, it returns the bijective Radon-Nikodym derivative $f_X$ computed w.r.t. $P$.</p>

<p>Paul Lévy showed that $F_X(b) - F_X(a) + {1\over2}P(X=a)-{1\over2}P(X=b) = \lim_{T\to\infty} {1\over2\pi} \int_{-T}^{T} {e^{-itb} - e^{-ita} \over it} \varphi(t)\, dt$, where $a,b\in\mathbb{R}$ and $a &lt; b$. The pmfs in the equation vanish if $a$ and $b$ are contained in the <a href="https://en.wikipedia.org/wiki/Continuity_set">continuity set</a> $C(F_X) = \lbrace x: F_X(x) \text{ is continuous at } x \rbrace$, and we have $F_X(b) - F_X(a) = \lim_{T\to\infty} {1\over2\pi} \int_{-T}^{T} {e^{-itb} - e^{-ita} \over it} \varphi(t)\, dt$. We may rewrite ${F(x+h)-F(x-h)\over{2h}} = {1\over{2\pi}}\int_{\mathbb{R}} {\sin{ht}\over{ht}}e^{-itx}\varphi_X(t)\, dt$ to gain power in numerical computations. While the equations in various forms showcase the univariate case, their extension to the multivariate case is achievable through a chain of integrals with respect to $P$, and also $\operatorname{FT}^{-1}$ can be obtained via the equations.</p>

<p>// Furthermore, J. Gil-Pelaez states that $F_X(x) = {1\over{2}} + {1\over{2\pi}} \int_{0}^{\infty} {e^{itx}\varphi(-t) - e^{-itx}\varphi(t) \over it}\, \mathrm{d}t = {1\over{2}} - {1\over{2\pi}} \int_{0}^{\infty} {\operatorname{Im}[e^{-itx}\varphi_X(t)] \over t}\, \mathrm{d}t$, where $x \in \mathbb{R}$ is a continuity point of $F_X$ <strong>(#3)</strong>. ____NotReadThePaperYet</p>

<h2 id="iii">III</h2>
<hr />
<p>Characteristic functions find applications in modern machine learning, exemplified by Google’s development of the <a href="https://arxiv.org/abs/2009.14794">Performer (2020)</a>, which utilises <a href="https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf">Random Features (2007)</a> to incorporate linearly scalable attention matrices, thus facilitating faster training on extended sequences. The expensive cost for inner product operations is reduced by applying the <a href="https://en.wikipedia.org/wiki/Bochner%27s_theorem">Bochner’s theorem</a> on $k(\mathbf{x},\mathbf{y}) = \phi(\mathbf{x})^{\top} \phi(\mathbf{y})$, where $k:\mathbb{R}^n \to \mathbb{R}$ is a kernel, $\phi:\mathbb{R}^n\to\mathbb{R}^m$ is a feature map, and $m \geq n$ <strong>(#4)</strong>. That is, the authors attempted to approximate an attention matrix requiring a quadratic complexity of $\mathcal{O}(n^2)$, by using a randomised feature map $z: \mathbb{R}^n \to \mathbb{R}^m$, where $m \leq n$.</p>

<p>Specifically, given a <a href="https://en.wikipedia.org/wiki/Definite_matrix">positive-semidefinite matrix</a> $A_{n \times n} = (a_{qr})_{q,r=1}^{n}$ with $a_{qr}=g(x_q - x_r)$ for every $x \in \mathbb{R}^n$, we call $g:\mathbb{R} \to \mathbb{C}$ a <a href="https://en.wikipedia.org/wiki/Positive-definite_function">positive-definite function</a>, and a positive-definiteness of $g$ arises if there exists $f \geq 0$ such that $g = \operatorname{FT}\lbrace f \rbrace$. Therefore, if one has a continuous positive-definite function $\phi: \mathbb{R} \to \mathbb{C}$ with $\phi(0) = 1$, then there always exists a Borel probability measure $\mu:\mathbb{R^n} \to [0,1]$ such that $\phi = \operatorname{FT}\lbrace \mu \rbrace$. This result, called the <a href="https://en.wikipedia.org/wiki/Bochner%27s_theorem">Bochner’s theorem</a> asserts that $p(\omega) = {1\over{2\pi}}\int_{\mathbb{R}} e^{-i\omega^{\top}(\mathbf{x}-\mathbf{y})} k(\mathbf{x}-\mathbf{y})\, \mathrm{d}\mathbf{\triangle}$ is a proper distribution function, if the shift-invariant kernel $k(\mathbf{x},\mathbf{y})=k(\mathbf{x}-\mathbf{y})$ can be properly scaled to have $k(0) = 1$.</p>

<p>The inversion formula reverts $k(\mathbf{x} − \mathbf{y}) = \int_{\mathbb{R}^n} e^{i\omega^{\top}(\mathbf{x-\mathbf{y}})}p(\omega)\, \mathrm{d}\omega$ and we rewrite $k(\mathbf{x} − \mathbf{y}) = \operatorname{E}[\zeta_{\omega}(\mathbf{x})\zeta_{\omega}(\mathbf{y})^{\ast}]$ with $\zeta_{\omega}(\mathbf{x}) = e^{i\omega^{\top}\mathbf{x}}$, where $\zeta_{\omega}(\mathbf{x})\zeta_{\omega}(\mathbf{y})^{\ast}$ is an unbiased estimate of $k(\mathbf{x},\mathbf{y})$ when $\omega$ is drawn from $p(\omega)$. $k$ converges if $e^{\mathbf{x}} = \cos(\mathbf{x}) + i\sin(\mathbf{x})$ is replaced by $\cos(\mathbf{x})$, and so, if $p(\omega)$ and $k(\triangle)$ are real, then $k(\mathbf{x},\mathbf{y})=\operatorname{E}[z_{\omega}(\mathbf{x})z_{\omega}(\mathbf{y})]$, where $z_{\omega}(\mathbf{x}) = \sqrt{2}\cos(\omega^{\top}\mathbf{x} + b)$ and $b$ is drawn uniformly from $[0,2\pi]$. We can substitute $k(\mathbf{x}−\mathbf{y}) \approx z(\mathbf{x})^{\top}z(\mathbf{y})$ for $z$ to map samples onto a lower-dimensional space with small variances <strong>(#5)</strong>, leading faster inner product operations crucial for many machine learning algorithms (e.g. OLS, SVM, PCA).</p>

<h2>**</h2>
<hr />
<p><strong>(#1)</strong> The Fourier transform defined in both ways by using $e^{-itx}$ or $e^{itx}$ are essentially the same as we can call either $i$ or $-i$ the imaginary unit. <strong>(#2)</strong> $X_n$ is not necessarily on the common probability space.  <strong>(#3)</strong> The 2nd equality comes by $\operatorname{Im}(z) = (z-z^\ast) / {2i}$. <strong>(#4)</strong> $n$ denotes an input sequence size. <strong>(#5)</strong> Hoeffding’s inequality yields fast convergence in $m$: $P[{\vert z(\mathbf{x})^{\top}z(\mathbf{y}) − k(\mathbf{x},\mathbf{y}) \vert} \geq \varepsilon] ≤ 2\exp(−m\varepsilon^{2}/4)$, where $z(x)^{\top}z(\mathbf{y}) = \Sigma_{j=1}^{m}z_{\omega_j}(\mathbf{x})z_{\omega_j}(\mathbf{y})$.</p>

				
					<br>
<hr>
<br>
<div class="comment">
	<p>I gathered words solely for my own purposes without any intention to break the rigorosity of the subjects.<br>
	Well, I prefer eating corn in spiral <i class="fa fa-cutlery"></i>.</p>
</div>
				
			</div>
		</div>
    
	<script src="/assets/js/001.js"></script>
	<script src="/assets/js/002.js"></script>
	<div class="footer">
		<p>Code licensed under <a href="https://github.com/h01000110/h01000110.github.io/blob/master/LICENSE" target="_blank">MIT License</a></p>
	</div>
</body>
</html>