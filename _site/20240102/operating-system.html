<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
	<link rel="stylesheet" href="/assets/css/atom-one-light.css">
    
        <title>402. operating system</title>
		<link rel="stylesheet" type="text/css" href="/assets/css/002.css">
    
	<link rel="stylesheet" href="/assets/css/font-awesome.min.css">
	<link rel="shortcut icon" href="/assets/img/favicon.ico" type="image/x-icon">
	<link rel="icon" href="/assets/img/favicon.ico" type="image/x-icon">
	<script src="/assets/js/highlight.pack.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>
	
		<!--http://benlansdell.github.io/computing/mathjax/-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    extensions: [
      "tex2jax.js",
      "MathMenu.js",
      "MathZoom.js",
      "AssistiveMML.js",
      "a11y/accessibility-menu.js"
    ],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    jax: ["input/TeX", "output/CommonHTML"],
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
</script>
<!-- You may add the following into MathJax.Hub.Config:
CommonHTML: {
    scale: 85
}-->
<script type="text/javascript" id="MathJax-script" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
	
</head>
<body>
	<div class="wrapper">
		<div class="default_title">
			<img src="/assets/img/mycomputer.png" />
			
				<h1>Hikikomori</h1>
			
		</div>
		<ul class="topbar">
	<a href="/me"><li><u>A</u>bout</li></a>
	<a href="https://www.linkedin.com/in/sung-kim-28b78a7b/" target="_blank"><li><u>L</u>inkedIn</li></a>
	<!-- 
		<a href="http://soundcloud.com/h01000110" target="_blank"><li><u>S</u>oundcloud</li></a>
	-->
</ul>
		<div class="tag_list">
			<ul id="tag-list">
				<li><a href="/" ><img src="/assets/img/disk.png" />(C:)</a>
			<ul>
				
				
				<li><a href="/tag/cs300/" title="cs300"><img src="/assets/img/folder.ico" />cs300</a></li>
				
				<li><a href="/tag/cs400/" title="cs400"><img src="/assets/img/folder.ico" />cs400</a></li>
				
				<li><a href="/tag/math200/" title="math200"><img src="/assets/img/folder.ico" />math200</a></li>
				
				<li><a href="/tag/math500/" title="math500"><img src="/assets/img/folder.ico" />math500</a></li>
				
			</ul>
				</li>
			</ul>
		</div>
		<div class="post_list">
			
				<ul>
					
					<li><a href="/20240201/functional-analysis" title="501. functional analysis"><img src="/assets/img/file.ico" title="501. functional analysis" />501. functional analysis</a></li>
					
					<li><a href="/20240105/parallel-computing" title="Parallel Computing"><img src="/assets/img/file.ico" title="Parallel Computing" />Parallel Computing</a></li>
					
					<li><a href="/20240104/networking" title="Networking"><img src="/assets/img/file.ico" title="Networking" />Networking</a></li>
					
					<li><a href="/20240103/virtualisation" title="Virtualisation"><img src="/assets/img/file.ico" title="Virtualisation" />Virtualisation</a></li>
					
					<li><a href="/20240102/tmp" title="402. operating system"><img src="/assets/img/file.ico" title="402. operating system" />402. operating system</a></li>
					
					<li><a href="/20240102/operating-system" title="402. operating system"><img src="/assets/img/file.ico" title="402. operating system" />402. operating system</a></li>
					
					<li><a href="/20240101/computer" title="401. dl compilation"><img src="/assets/img/file.ico" title="401. dl compilation" />401. dl compilation</a></li>
					
					<li><a href="/20230101/ml-paper" title="301. ml literature"><img src="/assets/img/file.ico" title="301. ml literature" />301. ml literature</a></li>
					
					<li><a href="/20220112/markov-chain" title="212. markov chain"><img src="/assets/img/file.ico" title="212. markov chain" />212. markov chain</a></li>
					
					<li><a href="/20220111/martingale" title="211. martingale"><img src="/assets/img/file.ico" title="211. martingale" />211. martingale</a></li>
					
					<li><a href="/20220110/stochastic-process" title="210. stochastic process"><img src="/assets/img/file.ico" title="210. stochastic process" />210. stochastic process</a></li>
					
					<li><a href="/20220109/laws-of-the-iterated-logarithm" title="209. law of the iterated logarithm"><img src="/assets/img/file.ico" title="209. law of the iterated logarithm" />209. law of the iterated logarithm</a></li>
					
					<li><a href="/20220108/central-limit-theorem" title="208. central limit theorem"><img src="/assets/img/file.ico" title="208. central limit theorem" />208. central limit theorem</a></li>
					
					<li><a href="/20220107/laws-of-large-number" title="207. law of large number"><img src="/assets/img/file.ico" title="207. law of large number" />207. law of large number</a></li>
					
					<li><a href="/20220106/modes-of-convergence" title="206. mode of convergence"><img src="/assets/img/file.ico" title="206. mode of convergence" />206. mode of convergence</a></li>
					
					<li><a href="/20220105/characteristic-function" title="205. characteristic function"><img src="/assets/img/file.ico" title="205. characteristic function" />205. characteristic function</a></li>
					
					<li><a href="/20220104/inequality" title="204. inequality"><img src="/assets/img/file.ico" title="204. inequality" />204. inequality</a></li>
					
					<li><a href="/20220103/expectation" title="203. expectated value"><img src="/assets/img/file.ico" title="203. expectated value" />203. expectated value</a></li>
					
					<li><a href="/20220102/random-variable" title="202. random variable"><img src="/assets/img/file.ico" title="202. random variable" />202. random variable</a></li>
					
					<li><a href="/20220101/probability" title="201. probability theory"><img src="/assets/img/file.ico" title="201. probability theory" />201. probability theory</a></li>
					
				</ul>
			
		</div>
		<div class="post_total">
			
				<div class="left">20 object(s)</div>
			
			<div class="right">&nbsp;</div>
		</div>
	</div>
	
        <div class="content">
			<div class="post_title">
				<img src="/assets/img/file.png" />
				<h1>402. operating system</h1>
				<a href="/"><div class="btn"><span class="fa fa-times"></span></div></a>
				<div class="btn btn_max"><span class="fa fa-window-maximize"></span></div>
				<div class="btn"><span class="fa fa-window-minimize"></span></div>
			</div>
			<ul class="topbar">
				<li>January 2, 2024</li>
			</ul>
			<div class="post_content">
        		<h1 id="operating-system">Operating System</h1>

<hr />
<p>A digital computer is bare metal without the <a href="https://www.youtube.com/watch?v=26QPDBe-NB8">operating system</a> (OS) in which it is often taken for granted while being less than a century old yet underpins nearly all computing devices today. We inevitably encounter it when moving from high-level code down to low-level hardware instructions.</p>

<!-- https://pravin-hub-rgb.github.io/BCA/resources/sem2/operating_sys/index.html -->
<!-- https://www.jmeiners.com/lc3-vm/#:lc3.c -->
<!-- https://www.youtube.com/watch?v=ioJkA7Mw2-U -->
<!-- https://www.youtube.com/watch?v=xFMXIgvlgcY -->
<!-- https://youtu.be/eP_P4KOjwhs?si=gOPQIxLH6cQMk8vq -->

<!-- round robin, fifo, ... -->
<!-- If data is large or its size varies, we use heap, and in stack, we just maintain a reference (i.e. pointer) to the value.... The *malloc* function in C internally uses *mmap* to free up a dedicated space and reclaim the OS for reusability. *free* does .... By using linked list, we do not need large amounts of contiguous memory, although the this data structure leads to decreased probabilities of cache hits. If our aim is to maintain compactness in our list, what we need is an array list (e.g. an array wrapped in the C struct with relevant metadata) -->

<h2 id="i">I</h2>

<hr />

<h3 id="11-overview"><strong>1.1. Overview</strong></h3>

<p style="margin-bottom: 12px;"> </p>

<p>In the earliest generation of electronic computers (1940s‚Äì50s), machines such as the ENIAC were programmed manually in absolute machine code, often by rewiring circuits or feeding in <a href="">punched cards</a>. Programs ran in isolation, required laborious setup, and so took significant idle time between jobs. The concept of an operating system began to emerge in the 1950s with the introduction of <a href="">batch processing systems</a>, and a major milestone was <a href="">GM-NAA I/O</a> developed by General Motors (GM) for the <a href="">IBM 701</a>. It enabled users to group jobs into batches (i.e. while favoured homogeneity within batches), and execute them sequentially without manual intervention.</p>

<p>The 1960s witnessed a shift toward <a href="">multiprogramming</a> and <a href="">time-sharing systems</a> (TSS), which presented the <a href="">concurrent execution</a> of multiple programs residing in memory, supported by the CPU switching between them. It gave rise to the birth of <a href="">Multics</a>, a pioneering TSS designed to provide a robust, multi-user computing environment, and jointly built by AT&amp;T Bell Labs, GE, and MIT. Nevertheless, dissatisfaction with its complexity led researchers at Bell Labs to develop <a href="">Unix</a> in the early 1970s. This simper OS adopted a modular kernel, hardware abstraction, and multi-user support, while these principles still remain central to modern operating system design.</p>

<p>The 1980s ushered in the era of personal computing, shifting OS development from <a href="">command-line interfaces</a> (CLI) toward <a href="">graphical user interfaces</a> (GUI) to improve accessibility for non-technical users. Microsoft introduced <a href="">MS-DOS</a> in 1981, a single-tasking CLI-based OS, followed by successive versions of <a href="">Windows</a> that adopted cooperative and later preemptive multitasking. Concurrently, Apple‚Äôs <a href="">Macintosh OS</a> (macOS) brought the GUI into mainstream. In the 1990s, <a href="">Linux</a> emerged as a free and open-source Unix-like alternative. Rooted in Unix philosophy, Linux fuelled innovation across servers, mobile devices, and also embedded systems.</p>

<ul>
  <li>
    <iframe width="500" height="280" src="https://www.youtube.com/embed/kKJxzay85Vk?si=nemG0E7zqsjleTpG" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
  </li>
</ul>

<h3 id="12-operating-system"><strong>1.2. Operating System</strong></h3>

<p style="margin-bottom: 12px;"> </p>

<p>A modern OS enforces a strict separation between <a href="">user mode</a> (unprivileged) and <a href="">kernel mode</a> (privileged). This hardware-supported principle protects the system by preventing unprivileged programs from directly accessing critical hardware resources. For example, <a href="">application programs</a>, such as the text editor, are generally run as unprivileged processes and must rely on the services exposed by the OS (e.g. file I/O or memory allocation). <a href="">System programs</a> including shells, compilers, <a href="">daemons</a>, and init systems, that also reside in user space, provide runtime infrastructure which interprets user instructions and translates them into requests the kernel can fulfill.</p>

<p>As drawn below, the <a href="">application programming interface</a> (API) provided by standard libraries (e.g. libc on Unix-like systems) abstracts the complexity of invoking system calls directly from user mode. That is, when a user-space program requires privileged functionality (e.g. spawning a process), it leverages a high-level API routine which internally issues one or more <a href="">system calls</a>. These serve as well-defined entry points into the kernel, usually implemented through software interrupts, trap instructions, or CPU-specific mechanisms. This controlled access is generally preferred to ensure only trusted kernel code can modify hardware or access protected memory.</p>

<p>While APIs define the data structures and function signatures available to programmers, the <a href="https://stackoverflow.com/questions/3784389/difference-between-api-and-abi">application binary interface</a> (ABI) governs how a compiled program communicates with the OS at the binary level. It specifies calling conventions (i.e. how params passed from the program to the OS - typically via registers or the stack), register usage, and system call invocation method. ABI differences also encompass executable formats - {Linux: <a href="">executable and linkable format</a> (ELF), Windows: <a href="">portable executable</a> (PE)}, directory layouts, process models, and available runtime libraries. As a result, most programs are not only architecture-specific but also OS-dependent.</p>

<ul>
  <li>
    <div style="position: relative; display: inline-block; background-color: white;"> <img src="../assets/blog/2024-01-02-api_vs_abi.png" width="500" /> <a href="https://www.sciencedirect.com/topics/computer-science/application-binary-interface" target="_blank" style="position: absolute; bottom: -8px; right: 4px; font-size: 12px;">[src]</a> </div>
  </li>
</ul>

<p>On Linux running on <a href="">x86-64</a> (i.e. the Intel and AMD CPU architecture), for instance, when calling <em>write()</em> via the C standard library to output data to a file, the predefined system call which executes in kernel mode to perform the actual operation is invoked by placing the ‚Äúsyscall number‚Äù (e.g. 1) in the <a href="https://www.cs.uaf.edu/2017/fall/cs301/lecture/09_11_registers.html">rax</a> register, while its args - (‚Äúfile descriptor‚Äù, ‚Äúbuffer pointer‚Äù, ‚Äúbyte count‚Äù) are passed via <a href="">rdi</a>, <a href="">rsi</a>, and <a href="">rdx</a>, respectively. In contrast, Windows uses <em>WriteFile()</em> with a distinct ABI and system call interface. Cross-platform compatibility of softwares is usually achieved by standardised APIs (e.g. POSIX) or the use of portability layers (e.g. the JVM or Python interpreter). <!-- which abstract away OS-specific details. --></p>

<pre><code>section .data
    msg  db "Hello", 10 ; "Hello\n"
    msg_len equ $ - msg  ; Length of the message

section .text
    global _start

_start:
    mov  rax, 1   ; syscall number for write
    mov  rdi, 1   ; file descriptor (stdout)
    mov  rsi, msg   ; pointer to buffer
    mov  rdx, msg_len  ; number of bytes
    syscall      ; invoke kernel

    ; Exit the program (syscall 60)
    mov  rax, 60   ; syscall number for exit
    xor  rdi, rdi   ; exit code 0
    syscall
</code></pre>

<h3 id="13-shell--kernel"><strong>1.3. Shell &amp; Kernel</strong></h3>

<p style="margin-bottom: 12px;"> </p>

<p>Accordingly, interaction with the OS kernel often occurs through two common interfaces: i) standard libraries that wrap system calls; ii) <a href="">shells</a> which act as command interpreters; In both cases, transitions from user mode to kernel mode are necessary for any privileged operations. Specifically, the <a href="https://www.josehu.com/technical/2021/05/24/os-kernel-models.html">kernel</a>, which operates at the highest privilege level, forms the operating system‚Äôs core and mediates all access to hardware and protected resources, while the shell serves as the outermost user-facing interface. Note that the dual-mode architecture is enforced by hardware (e.g. using a mode bit), but both the kernel and shell themselves are implemented in software.</p>

<p>Advanced CLI-based shells including <a href="">Bash</a>, <a href="https://github.com/ohmyzsh/ohmyzsh/wiki/Cheatsheet">Zsh</a>, and <a href="">Fish</a> can support scripting, I/O redirection, job control, and process substitution. They parse user commands (e.g. <em>ls</em>, <em>ps</em>, <em>cat</em>), resolves the appropriate binaries, and initiates execution using system calls such as <em>fork()</em>, <em>exec()</em>, and <em>wait()</em>. <a href="">Terminal emulators</a> such as Mac Terminal merely host shell processes and should not be mistaken for the shell itself. On graphical systems, user interaction is instead mediated via desktop environments - {Linux: <a href="">GNOME</a>, macOS: <a href="">Finder</a>, Windows: <a href="">Explorer</a>} which provide a visual interface and invoke the same system calls and kernel services underneath. In fact, I use:</p>

<style>
pre {
  line-height: 1.5; /* 1.5 times the font size for spacing */
  white-space: pre; /* default, no wrap */
  max-width: none;  /* no max width */
  overflow-x: auto; /* horizontal scrollbar if too wide */
}
</style>

<pre>
  üñ•Ô∏è Emulator: iTerm2
  |
  +-- üêö Shell: Zsh
      |
      |     ‚Ä¢ ohmyzsh (framework for zsh configuration)
      |
      +-- üì¶ Package Manager: Homebrew
      |
      |     ‚Ä¢ lsd (ls deluxe - modern replacement for ls)
      |     ‚Ä¢ bat (better cat - enhanced syntax highlighting)
      |     ‚Ä¢ fzf (fuzzy finder)
      |     ‚Ä¢ fd, ripgrep, htop, etc.
      |
      +-- ‚úèÔ∏è Text Editor: Neovim
      |
      |     ‚Ä¢ Config: LazyVim
      |     ‚Ä¢ Plugins: custom configs/additions
</pre>

<p>While the kernel manages low-level operations such as CPU scheduling, memory management, IPC, and device I/O, its architectural design critically affects system performance, modularity, and fault tolerance. <a href="">Monolithic kernels</a> (e.g. Linux) bundle all core services into a single privileged binary, enabling fast in-kernel communication but increasing the risk of system-wide failure. <a href="">Microkernels</a> (e.g. seL4) retain only minimal services (e.g. scheduling) in kernel space, delegating others (e.g. file systems) to user space to improve modularity and fault isolation. Meanwhile, <a href="">hybrid kernels</a> (e.g. XNU in macOS) adopt a layered structure to reconcile these trade-offs.</p>

<!-- - <div style="position: relative; display: inline-block;"> <img src="https://minnie.tuhs.org/CompArch/Lectures/Figs/unixarch.gif" width="500"> <a href="https://minnie.tuhs.org/CompArch/Lectures/week07.html" target="_blank" style="position: absolute; bottom: -8px; right: 4px; font-size: 12px;">[src]</a> </div> -->

<!-- - <div style="position: relative; display: inline-block;"> <img src="https://i.namu.wiki/i/XlV7BIE7FBHBVD4Ad8wTobBo2yhfwqwtT1jGEeOKzyFDQwMtQsStlNVva40_P_RoMAgvnXx6SXFFTv33rXwsYoPem4hkvH4FujtBdd9iP_Zp2vlDbm4pIP-tsSAk-v2094NFOldeqqr14KQxyVGg1g.png" width="500"> <a href="https://namu.wiki/w/%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C" target="_blank" style="position: absolute; bottom: -8px; right: 4px; font-size: 12px;">[src]</a> </div> -->

<!-- - <div style="position: relative; display: inline-block;"> <img src="https://effective-shell.com/assets/images/diagram3-terminal-and-shell-31620f593a4c3838051a5a6dcea17577.png" width="500"> <a href="https://effective-shell.com/part-2-core-skills/what-is-a-shell/" target="_blank" style="position: absolute; top: 0px; left: 4px; font-size: 12px;">[src]</a> </div> -->

<!-- - <div style="position: relative; display: inline-block;"> <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSjzbd3THEk91Udium9DfdCeZaqtg1eJ1HTfw&s" width="500"> <a href="http://ibgwww.colorado.edu/~lessem/psyc5112/usail/concepts/anatomy-of-unix/anatomy.html" target="_blank" style="position: absolute; top: 0px; left: 4px; font-size: 12px;">[src]</a> </div> -->

<!-- https://velog.io/@juliejung98/%EC%89%98%EA%B3%BC-%EC%BB%A4%EB%84%90-Shell-Kernel -->

<ul>
  <li>
    <div style="position: relative; display: inline-block; background-color: white;"> <img src="https://leimao.github.io/images/blog/2021-06-18-Microkernel-VS-Monolithic-Kernel-OS/OS-structure.svg" width="500" height="250" /> <a href="https://leimao.github.io/blog/Microkernel-VS-Monolithic-Kernel-OS/" target="_blank" style="position: absolute; top: 0px; left: 4px; font-size: 12px;">[src]</a> </div>
  </li>
</ul>

<!-- - <div style="position: relative; display: inline-block;"> <img src="https://kuleuven-diepenbeek.github.io/osc-course/img/OS-structure2.svg" width="500" height="140"> <a href="https://kuleuven-diepenbeek.github.io/osc-course/ch1-introos/intro-os/" target="_blank" style="position: absolute; top: 0px; left: 4px; font-size: 12px;">[src]</a> </div> -->

<h3 id="14-system-call"><strong>1.4. System Call</strong></h3>

<p style="margin-bottom: 12px;"> </p>

<p>TODO‚Ä¶</p>

<!-- - <iframe width="500" height="280" src="https://www.youtube.com/embed/H4SDPLiUnv4?si=ml8bT-7fhG9_0xkU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe> -->

<!-- strace to monitor system calls -->
<!-- https://www.youtube.com/watch?v=19_vVxmTfPg -->
<!-- System calls in modern OS are the primary mechanism by which user-space programs request services from the kernel. Because direct access to hardware and critical resources is prohibited in user mode, programs must rely on the kernel to perform operations such as file I/O, memory allocation, process creation, and network communication. These requests are issued through system calls, which act as controlled entry points into kernel space. High-level programming interfaces provided by standard libraries (e.g. [libc](), [glibc](), or language runtimes such as Python‚Äôs *os* module) abstract these calls into user-friendly functions, translating them into low-level instructions that prepare the necessary registers and trigger the transition into kernel mode.

This transition is supported by [dual-mode]() CPU operation, which enforces a strict boundary between user mode and kernel mode. When a system call is invoked‚Äîon x86-64, typically using the syscall instruction‚Äîthe processor switches from [user mode]() to [kernel mode](), saving the user context and transferring control to a predefined system call handler in the kernel. The system call number (usually passed in a designated register) determines which kernel routine to execute, and the accompanying arguments are validated for correctness and security. After the operation completes, the kernel restores the original execution context and returns to user mode, resuming the program. This architecture ensures that while applications can access powerful system capabilities, they do so through a carefully controlled interface that preserves system integrity, prevents unauthorized access, and isolates faults. -->

<!-- In modern OS, user-space software must invoke kernel functionality via [system calls]()‚Äîentry points into the kernel provided by standard libraries such as *libc*, *glibc*, or language-specific runtimes (e.g. Python‚Äôs *os*, *sys*). These libraries translate user-friendly API calls into low-level requests to the kernel. The CPU enforces this boundary via dual-mode operation: [user mode]() restricts direct hardware access, while [kernel mode]() permits it. A system call triggers a [context switch]() to kernel mode; after execution, control returns to the calling process. This controlled handoff ensures safety, enforces isolation, and maintains system integrity. -->

<!-- System calls are the primary interface (i.e. APIs exposed by the kernel) through which user-space programs request services from the kernel. Because user programs run in restricted user mode, they are not permitted to directly access hardware or critical system resources. Instead, when a program needs to perform a privileged operation‚Äîsuch as reading from a file, allocating memory, or creating a new process‚Äîit issues a system call. This triggers a controlled context switch into kernel mode, where the requested operation is validated and executed. Once the operation is complete, control is returned to the user process along with any relevant results or error codes. This mechanism ensures both safety (by enforcing privilege boundaries) and consistency (by standardizing access to resources). -->

<!-- 
To simplify system call usage, most application developers interact with them indirectly through standard libraries like libc, glibc, or language runtimes. For instance, when a C program calls fopen(), the library internally prepares arguments, invokes the lower-level open() system call, and handles return values. This layered abstraction hides architecture-specific details (e.g., syscall numbers, register conventions) from the programmer, enabling portability and maintainability. Behind the scenes, invoking a system call typically involves placing parameters in CPU registers and executing a trap instruction‚Äîsuch as syscall on x86_64 or svc on ARM‚Äîthat transitions the CPU into kernel mode at a predefined entry point.

System calls can be grouped into several broad categories, reflecting the types of services the OS provides. Process control calls (fork(), exec(), wait()) handle creation and lifecycle management of processes. File operations (read(), write(), open(), close()) enable programs to perform I/O on file descriptors abstracting real or virtual files. Memory management calls (mmap(), brk()) govern the allocation and mapping of address space. Device and network I/O are handled through calls like ioctl() and socket(). There are also informational calls (getpid(), uname()) that report system state. By keeping this interface minimal and stable, the kernel enforces strict control over hardware access while allowing rich functionality in user space, supporting everything from desktop applications to network servers and container runtimes.

- <div style="position: relative; display: inline-block; background-color: white;"> <img src="https://pravin-hub-rgb.github.io/BCA/resources/sem2/images/ker4.svg" width="500" height="350"> <a href="https://pravin-hub-rgb.github.io/BCA/resources/sem2/operating_sys/intro/index.html" target="_blank" style="position: absolute; bottom: -8px; right: 4px; font-size: 12px;">[src]</a> </div> -->

<!-- https://pravin-hub-rgb.github.io/BCA/resources/sem2/images/typesc.svg -->

<!-- - <iframe width="500" height="280" src="https://www.youtube.com/embed/eP_P4KOjwhs?si=wDkkO45KIt-r8pln" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe> -->

<!-- - <div style="position: relative; display: inline-block;"> <img src="../assets/blog/2024-01-02-dual_mode.png" width="500"> <a href="https://velog.io/@ongddree/%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C-%EC%9D%B4%EC%A4%91-%EB%8F%99%EC%9E%91-%EB%AA%A8%EB%93%9COS-dual-mode-operation" target="_blank" style="position: absolute; bottom: -8px; right: 4px; font-size: 12px;">[src]</a> </div>\ -->

<!-- ÏãúÏä§ÌÖú ÏΩúÏù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Ï≤òÎ¶¨ÎêòÎ©¥ Ïª§ÎÑêÏùÄ IRET(Interrupt Return) Î™ÖÎ†πÏñ¥Î•º ÏÇ¨Ïö©ÌïòÏó¨ Ïª§ÎÑê Î™®ÎìúÏóêÏÑú ÏÇ¨Ïö©Ïûê Î™®ÎìúÎ°ú Ï†ÑÌôòÌïòÍ≥† ÏùëÏö© ÌîÑÎ°úÍ∑∏Îû®Ïù¥ ÏãúÏä§ÌÖú ÏΩúÏùÑ Ìò∏Ï∂úÌïú ÏúÑÏπòÎ°ú ÎèåÏïÑÍ∞ÑÎã§. -- https://velog.io/@ongddree/%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C-%EC%9D%B4%EC%A4%91-%EB%8F%99%EC%9E%91-%EB%AA%A8%EB%93%9COS-dual-mode-operation -->

<!-- 
### **1.4. OS Virtualisation**
<p style="margin-bottom: 12px;"> </p>

- https://insights.sei.cmu.edu/blog/virtualization-via-containers/

- https://blog.bytebytego.com/p/what-are-the-differences-between

- https://stackoverflow.com/questions/71955661/what-is-the-difference-between-containers-and-process-vms-not-system-vms  

Virtualization abstracts hardware resources, enabling multiple virtual systems to run on a single physical machine as though each were operating independently. It improves resource utilization, isolation, and scalability, forming the backbone of modern computing environments. Hardware virtualization, achieved through hypervisors, is a key approach. Type 1 hypervisors like VMware ESXi run directly on hardware for high performance, while Type 2 hypervisors like VirtualBox operate atop an existing OS, providing ease of use at the cost of efficiency.

OS-level virtualization, exemplified by containers like Docker, isolates applications within lightweight environments that share the host OS kernel. Containers are faster to deploy and consume fewer resources than virtual machines, making them ideal for microservices and cloud-native applications. However, their reliance on a shared kernel requires robust isolation mechanisms to maintain security.

- <div style="position: relative; display: inline-block;"> <img src="https://insights.sei.cmu.edu/media/images/fourthpost_firstgraphic_092520.max-1280x720.format-webp.webp" width="500"> <a href="https://insights.sei.cmu.edu/blog/virtualization-via-containers/" target="_blank" style="position: absolute; top: 0px; left: 4px; font-size: 12px;">[src]</a> </div> -->

<!-- Containersiation (i.e. OS-level virtualisation using a conainer) is ... || https://blog.bytebytego.com/p/what-are-the-differences-between -->

<h2 id="ii">II</h2>

<hr />

<h3 id="21-unix"><strong>2.1. Unix</strong></h3>

<p style="margin-bottom: 12px;"> </p>

<p>Looking back at its origins, Unix emerged in 1969 at Bell Labs, when Ken Thompson and Dennis Ritchie repurposed a spare <a href="">PDP-7</a> 18-bit minicomputer to develop a lightweight, interactive operating system. Initially dubbed ‚ÄúUnics‚Äù (i.e. a pun on the earlier Multics), the system abandoned the complexity of its predecessor in favour of simplicity and modularity. Its adoption of a <a href="">hierarchical file system</a> (HFS), segmented memory, dynamic linking, and a minimal yet powerful API, demonstrate a new design philosophy that is focused on composability, portability, and clear separation of concerns between kernel-level mechanisms and user-space utilities.</p>

<p>A foundational abstraction in Unix was its uniform treatment of input/output. By representing all I/O resources (e.g. files, devices, and IPC endpoints) as file descriptors, Unix allowed disparate resources to be accessed via the same read/write interface. This ‚Äúeverything is a file‚Äù model, combined with the system‚Äôs use of plain-text configuration and output, led the environment very scriptable. Programs inherently were designed as small, single-purpose utilities that could be chained together using <a href="">pipes</a> (|). For example, the command <em>cat log.txt | grep error | sort | uniq -c</em> reads a log file, filters lines containing ‚Äúerror,‚Äù sorts them, and collapses duplicates into counts.</p>

<p>Another notable contribution was indeed portability. In the early 1970s, Unix was rewritten from assembly into the newly developed <a href="https://seriouscomputerist.atariverse.com/media/pdf/book/C%20Programming%20Language%20-%202nd%20Edition%20(OCR).pdf">C programming language</a> that was also created by Ritchie at Bell Labs. C evolved from the earlier <a href="">B programming language</a> (i.e. derived from BCPL) and introduced key features such as typed variables, structured control flow, and more direct memory manipulation. This decoupling from machine-specific assembly code enabled Unix to be recompiled on a wide variety of hardware platforms, marking it as the first widely portable operating system, and the co-evolution of Unix and C unlocked a generation of system-level programming. <!-- make it shorter* --></p>

<ul>
  <li>
    <iframe width="500" height="280" src="https://www.youtube.com/embed/tc4ROCJYbm0?si=HTFkF_s-YHnPd35_" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
  </li>
</ul>

<p>As AT&amp;T was restricted by the 1934 <a href="">Communications Act</a> and a 1956 antitrust consent decree from entering in commercial computing, Unix was freely or cheaply distributed and widely adopted in academia. One of its most influential offshoots was the <a href="">Berkeley software distribution</a> (BSD), launched in the late 1970s by Bill Joy and the <a href="">Computer Systems Research Group</a> (CSRG) at UC Berkeley. BSD began as a set of enhancements to AT&amp;T Unix, but evolved into a full OS by the late 1980s. With DARPA funding, BSD merged key networking features including the first complete TCP/IP stack, and became a reference platform for early <a href="">Internet</a> development.</p>

<p>BSD‚Äôs permissive licence and technical maturity attracted commercial interest throughout the 1980-90s. Its code was incorporated into systems such as SunOS by Sun Microsystems, Ultrix by Digital Equipment Corporation, and NeXTSTEP by NeXT Inc., and it laid the groundwork for enduring open-source projects including FreeBSD, NetBSD, OpenBSD, and DragonFly BSD. Its influence extended to modern platforms as Apple‚Äôs <a href="">Darwin</a>, the Unix core of macOS and iOS, is based on FreeBSD. Microsoft integrated BSD-derived code into Windows networking, and its legacy persists in routers, embedded appliances, and gaming consoles such as the <a href="">PlayStation 5</a>.</p>

<p>As Unix variants proliferated, differences in system calls, utilities, and behaviours hindered software portability and interoperability. To resolve this, the IEEE introduced the <a href="">Portable Operating System Interface</a> (POSIX) standard in the late 1980s, specifying a consistent API, shell command set, and utility behaviours for Unix-like systems. Although POSIX did not fully unify all implementations (especially proprietary extensions), it established a solid baseline that greatly improved cross-platform compatibility. This standardisation not only helped unify the fragmented Unix landscape but also influenced the development of modern OS, including Linux and BSD derivatives.</p>

<ul>
  <li>
    <div style="position: relative; display: inline-block; background-color: white;"> <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/77/Unix_history-simple.svg/2560px-Unix_history-simple.svg.png" width="500" /> <a href="https://en.wikipedia.org/wiki/Unix-like" target="_blank" style="position: absolute;  bottom: -8px; right: 4px; font-size: 12px;">[src]</a> </div>
  </li>
</ul>

<!-- - <div style="position: relative; display: inline-block;"> <img src="https://i.namu.wiki/i/af_1VZUEG31QudreEWCK26cD48GtRjNMZs7lZHwt11YpYot2vfLhkNp21lsbmnHGXlUtFVE5C-QrLo_E5EYCI_Q5yqa580UIYd6elP38702QFu3h-OOyInfG3dD3ZbH-lzx9BzYKEi4j4OC_ynE0NA.svg" width="500" height="320"> <a href="https://namu.wiki/w/Unix" target="_blank" style="position: absolute; bottom: -8px; right: 4px; font-size: 12px;">[src]</a> </div> -->

<!-- - <iframe width="500" height="280" src="https://www.youtube.com/embed/HADp3emVABg?si=slBlmD7_ktsw0__u" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe> -->

<h3 id="22-linux"><strong>2.2. Linux</strong></h3>

<p style="margin-bottom: 12px;"> </p>

<p>TODO‚Ä¶</p>

<!-- ### **2.2. Linux**
<p style="margin-bottom: 12px;"> </p>

Linux began in 1991 as a personal project by Linus Torvalds to develop a free, Unix-like kernel for the Intel 80386 architecture. Inspired by MINIX and licensed under the GPL, it quickly attracted contributions from developers worldwide and was paired with the GNU Project‚Äôs user-space tools to form a complete open-source operating system. Unlike proprietary Unix systems, which were tied to specific vendors, Linux grew through a decentralized, community-driven model and rapidly gained adoption across academia, hobbyist circles, and eventually industry.

Architecturally, Linux uses a monolithic kernel, integrating core services such as process scheduling, virtual memory, networking, and file systems into a single privileged binary. To balance flexibility and modularity, it introduced support for loadable kernel modules (LKMs), allowing dynamic insertion of drivers and extensions at runtime. Written in portable C with clean hardware abstraction layers, Linux was quickly ported to multiple architectures beyond x86, including ARM, PowerPC, and SPARC. Over time, it incorporated advanced features such as control groups (cgroups), namespaces, epoll-based I/O, and pluggable schedulers, making it well-suited for containers, cloud platforms, and embedded systems.

While Linux is not derived from any specific Unix source tree, it closely follows POSIX standards and Unix design principles, enabling compatibility with established Unix software. By the early 2000s, it had become the dominant OS for servers and infrastructure, displacing proprietary Unix in many environments. Today, Linux powers a wide range of systems‚Äîfrom Android smartphones and embedded IoT devices to enterprise data centers and all Top500 supercomputers‚Äîmaking it the most widely deployed Unix-like kernel in the world.

<iframe width="500" height="280" src="https://www.youtube.com/embed/E0Q9KnYSVLc?si=Fere9hvODg0z0MtB" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### **2.3. Linux FHS**
<p style="margin-bottom: 12px;"> </p>

As Linux distributions proliferated in the 1990s, inconsistencies in directory layouts across systems became a barrier to software portability and system maintenance. To address this, the Filesystem Hierarchy Standard (FHS) was introduced by the Linux Foundation to define a common directory structure and file placement for Unix-like systems. Although not universally enforced, most mainstream distributions‚Äîsuch as Debian, Fedora, and Arch‚Äîfollow FHS conventions to varying degrees, ensuring predictability for users, package managers, and software developers.

Under FHS, the root directory / serves as the top-level namespace, from which all other paths descend. Essential binaries and libraries needed for booting and single-user mode reside in /bin, /sbin, and /lib, while user home directories are located under /home, and configuration files under /etc. The /usr hierarchy holds secondary programs and libraries not required during early boot, and /var stores variable data such as logs, caches, and spool files. Temporary files reside in /tmp, and system-wide device files are represented under /dev, consistent with Unix‚Äôs ‚Äúeverything is a file‚Äù philosophy.

This structure helps separate static and dynamic data, as well as system and user space, which in turn supports package management, backups, and cross-distribution compatibility. Some directories‚Äîsuch as /run for volatile runtime data or /srv for service data‚Äîreflect modern additions to accommodate newer usage patterns. While some distributions experiment with alternative layouts (e.g. /usr merge), adherence to FHS simplifies development, scripting, and administration across diverse Linux environments, particularly in multi-user or production systems.

### **2.4. Linux Distribution**
<p style="margin-bottom: 12px;"> </p>

A Linux distribution (or ‚Äúdistro‚Äù) bundles the Linux kernel with a curated set of user-space utilities, libraries, configuration defaults, and package management tools to form a complete operating system. As the Linux kernel alone is insufficient for end users, distributions emerged to provide usable environments tailored to various audiences‚Äîranging from desktop users and system administrators to developers, embedded engineers, and cloud providers. Early distributions such as Slackware (1993), Debian (1993), and Red Hat Linux (1995) laid the groundwork for today‚Äôs ecosystem.

Each distribution makes distinct choices in areas such as init systems (e.g. systemd, OpenRC), packaging formats (e.g. .deb, .rpm, source-based), file system layout, release cadence, and included software stacks. For example, Debian emphasizes stability and is widely used as a base for derivatives like Ubuntu, which targets usability and long-term support for both desktops and servers. Red Hat Enterprise Linux (RHEL), and its derivatives like CentOS and AlmaLinux, prioritize commercial support and certification for enterprise workloads, while Arch Linux focuses on minimalism, rolling releases, and user control.

Distributions also diverge in tooling and update strategies. Package managers like apt, dnf, and pacman streamline software installation and system updates, while meta-tools like snap or flatpak aim to standardize application delivery across distros. Despite differences, most distributions remain interoperable through shared adherence to standards like POSIX, FHS, and the Linux Standard Base (LSB). As such, the choice of distribution often reflects the target use case, administrative preferences, or hardware constraints, rather than incompatibilities in the underlying Linux system.
 -->

<h2 id="iii">III</h2>

<hr />

<h3 id="31-process-management"><strong>3.1. Process Management</strong></h3>

<p style="margin-bottom: 12px;"> </p>

<p>Each process executes within its own isolated virtual <a href="">address space</a>, comprising distinct code, data, heap, and stack segments. This isolation ensures protection between processes and underpins system stability. The OS kernel maintains per-process metadata via a structure, known as the <a href="">process control block</a> (PCB), which contains its identifiers (PID/PPID), execution state, CPU register, memory mappings, scheduling parameters, and open file descriptors. A process advances through a <a href="">life cycle</a>: new (creation), ready (queued for CPU), running (actively scheduled), waiting (blocked on I/O, synchronisation, or an event), and terminated (completed or killed).</p>

<p>The <a href="">CPU scheduler</a>, a core part of the kernel, manages transitions between the mutually exclusive states and determines which ready process to dispatch next. Classical scheduling algorithms may include i) <a href="https://imbf.github.io/computer-science(cs)/2020/10/18/CPU-Scheduling.html">round robin</a>: fixed time slices; ii) <a href="">priority scheduling</a>: fixed or dynamic priority queues; iii) <a href="">multi-level feedback queues</a> (MLFQ): dynamically adjusts priorities based on process behaviours. In fact, <a href="">context switching</a> follows process scheduling in which this incurs overhead from cache disruption, <a href="">translation lookaside buffer</a> (TLB) flushes, and memory synchronisation. <!-- Modern operating systems such as Linux use the [completely fair scheduler]() (CFS), which maintains a red-black tree to distribute CPU time proportionally by tracking each process‚Äôs virtual runtime. --> Indeed, processes on a single core can be executed concurrently via appropriate time-slicing.</p>

<!-- - <div style="position: relative; display: inline-block;"> <img src="https://media.geeksforgeeks.org/wp-content/uploads/20231201161329/Process-Scheduler.png" width="500"> <a href="https://www.geeksforgeeks.org/operating-systems/process-schedulers-in-operating-system/" target="_blank" style="position: absolute;  bottom: -8px; right: 4px; font-size: 12px;">[src]</a> </div> -->

<!-- - <div style="position: relative; display: inline-block;"> <img src="https://media.geeksforgeeks.org/wp-content/uploads/20210615183559/Context_Switching.png" width="500" height="300"> <a href="https://www.geeksforgeeks.org/operating-systems/context-switch-in-operating-system/" target="_blank" style="position: absolute;  bottom: -8px; right: 4px; font-size: 12px;">[src]</a> </div> -->

<p>As systems can execute multiple <a href="">cooperating processes</a>, scheduling should be complemented by <a href="">inter-process communication</a> (IPC) methods (e.g. pipes, queues, or shared memory buffers) to enable coordination across isolated address spaces. <a href="">Message passing</a> delegates data transfer to the kernel via abstractions such as pipes, UNIX domain sockets, or System V message queues. Whereas, <a href="">shared memory</a> provides high-throughput, low-latency communication (e.g. NumPy arrays) but requires explicit <a href="">synchronisation</a>. The former is generally slower due to data copying and kernel involvement, but it well-simplifies coordination and improves fault isolation.</p>

<ul>
  <li>
    <div style="position: relative; display: inline-block; background-color: white"> <img src="https://notes.shichao.io/apue/figure_15.1.png" width="500" height="215" /> <a href="https://notes.shichao.io/apue/ch15/" target="_blank" style="position: absolute;  bottom: -8px; right: 4px; font-size: 12px;">[src]</a> </div>
  </li>
</ul>

<p><a href="">Multiprocessing</a> refers to concurrent execution of separate processes, each with its own memory space, generally mapped to different CPU cores and coordinated via IPC. For instance, PyTorch‚Äôs <a href="https://discuss.pytorch.org/t/efficient-gpu-data-movement/63707">DataLoader</a> relies on Python‚Äôs <em>multiprocessing</em> module <!--, with GPU tensor sharing between worker processes supported through CUDA-aware IPC, --> to parallelise data loading and improve input throughput. In distributed training, <em>torch.distributed</em> with <a href="">DistributedDataParallel</a> (DDP) spawns one process per device (e.g. CPU/GPU) and synchronises gradients at each backward pass. DDP needs communication backends (i.e. IPC layer) such as Gloo for CPUs and <a href="">NVIDIA collective communication library</a> (NCCL) for GPUs to coordinate model parameters and gradients.</p>

<p><!-- employing communication backends such as [Gloo]() or [NCCL](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/overview.html). PyTorch also extends shared memory with CUDA-aware IPC for GPU tensor sharing.  --></p>

<!-- . PyTorch also extends shared memory support through CUDA-aware IPC to enable GPU tensor sharing. Specifically,  -->

<!-- Complementary tools like Numba offer thread- or process-level parallelism through JIT compilation to machine code. -->

<!-- Multiprocessing enables the concurrent execution of multiple processes, each with its own isolated memory space and usually mapped to separate CPU cores. Since memory is not shared by default, coordination between processes must be handled using inter-process communication (IPC). In PyTorch, the DataLoader uses Python‚Äôs multiprocessing module to spawn worker processes that load and transform batches concurrently, improving input pipeline performance. For distributed training, PyTorch provides the torch.distributed package and its DistributedDataParallel (DDP) wrapper, which enables multiple processes‚Äîeach with its own model replica‚Äîto compute gradients independently and synchronize them efficiently across processes after each backward pass. This design scales well across both multi-core CPUs and multi-GPU systems, with DDP using shared memory or CUDA-aware communication backends (like NCCL) to reduce synchronization overhead. Complementary approaches such as Numba offer thread- and process-level parallelism through JIT compilation to machine code, enabling further parallel acceleration in custom numerical routines. -->

<p>In networked IPC, processes communicate over <a href="">sockets</a> identified by <a href="">IP addresses</a> and <a href="">ports</a>, which act as logical <a href="">endpoints</a> directing traffic to the correct process. For example, web servers typically use port 80 (HTTP) or 443 (HTTPS), while SSH uses port 22. The OS maps ports to socket endpoints and manages connection queues to deliver <a href="">packets</a> to the correct process. PyTorch also uses specific ports (e.g. <a href="https://tutorials.pytorch.kr/intermediate/dist_tuto.html">MASTER_PORT</a>) for synchronisation during distributed training, normally over <a href="">transmission control protocol</a> (TCP) or NCCL. As such, this networking concept remains central to distributed systems in the modern ML era.</p>

<ul>
  <li>
    <div style="position: relative; display: inline-block; background-color: white"> <img src="https://pylessons.com/media/Tutorials/YOLO-tutorials/YOLOv4-TF2-multiprocessing/1.png" width="500" height="220" /> <a href="https://pylessons.com/YOLOv4-TF2-multiprocessing" target="_blank" style="position: absolute;  bottom: -8px; right: 4px; font-size: 12px;">[src]</a> </div>
  </li>
</ul>

<p><a href="">Threads</a> are lightweight execution units within a process that share the same virtual address space. This shared context enables fine-grained parallelism with lower memory overhead and faster context switches compared to processes. Most modern OSes implement the $1 \colon 1$ threading model (e.g. <a href="">native POSIX threads library</a>) for each user thread to be directly mapped to a kernel thread, while $m \colon 1$ and $m \colon n$ (e.g. <a href="">Go</a> programming language) models are less widely adopted.  Threads are independently scheduled by the kernel and require synchronisation primitives such as <a href="">mutexes</a>, <a href="">spinlocks</a>, and condition variables to prevent <a href="">race conditions</a> and have stability.</p>

<p>The thread-level parallelism in CPython has long been constrained by the <a href="https://www.youtube.com/watch?v=dyhKXCpkCGE">global interpreter lock</a> (GIL), which is a mutex that serialises the execution of Python bytecode (i.e. one thread at a time) and was primarily implemented for <a href="">reference counting</a> involved in <a href="">garbage collection</a>, thereby limiting the effectiveness of threading for CPU-bound operations such as matrix multiplication. However, compute-intensive extensions in native C/C++ code (e.g. Numpy or PyTorch - see the <a href="https://discuss.pytorch.org/t/can-pytorch-by-pass-python-gil/55498">discussion</a>) can often <!-- temporarily --> release the GIL during execution, and thankfully, <a href="">PEP 703</a> introduces a build-time option to remove the GIL in Python 3.13 albeit with changes to the C API. <!-- Other Python implementation such as [Jython]() or [IronPython]() do not have a GIL. --></p>

<ul>
  <li>
    <iframe width="500" height="280" src="https://www.youtube.com/embed/M9HHWFp84f0?si=B3pvCInh5IWetRvf" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
  </li>
</ul>

<h3 id="32-memory-management"><strong>3.2. Memory Management</strong></h3>

<p style="margin-bottom: 12px;"> </p>

<p>Modern OS kernels manage memory through a combination of hardware-level and software-level techniques, primarily via <a href="">virtual memory</a> abstraction, and is responsible for how programs allocate, access, and release memory throughout their execution. This abstraction decouples logical memory seen by processes from the actual <a href="">physical memory</a>, enabling process isolation, efficient use of RAM, and safety against illegal memory access. Virtual memory is implemented using <a href="">page</a> tables that map virtual addresses to physical frames, and memory accesses are mediated by the <a href="">memory management unit</a> (MMU).</p>

<p>When a process accesses memory, the MMU translates virtual addresses into physical ones. If the requested page is not present in physical memory (i.e. <a href="">page fault</a>), the OS intervenes to load it from secondary storage (typically from a <a href="">swap file</a> or disk-backed mapping). Pages may be marked read-only, executable, or shared, allowing fine-grained access control. <a href="">Copy-on-write</a> (COW) mechanisms are commonly used to optimise memory usage, particularly in <em>fork()</em>-based process creation ‚Äî the child inherits the parent‚Äôs memory pages, which are duplicated only upon modification.</p>

<p>Segmentation divides memory into variable-length logical segments (e.g. code, stack, heap), while paging divides memory into fixed-size blocks. While segmentation enables logical grouping, it suffers from external fragmentation and complex protection schemes. Paging, by contrast, simplifies allocation and protection but incurs internal fragmentation and requires multi-level page tables to manage large address spaces. Most modern systems combine both by using paged segments or adopt flat address spaces with hierarchical page tables (e.g. x86-64‚Äôs 4-level paging).</p>

<p>In LLM inference engines such as vLLM, the concept of memory paging is elevated to manage GPU memory efficiently at scale. vLLM introduces <a href="">paged attention</a> to decouple memory allocation from attention key $K$ and value $V$ storage, enabling fine-grained, lazily allocated attention buffers. This reduces redundant memory copying and allows dynamic batching of input tokens, which significantly improves memory locality and throughput.</p>

<p>The OS kernel also handles dynamic memory allocation through system calls such as <em>brk()</em> and <em>mmap()</em>, with the latter being preferred in modern systems for allocating large or file-backed regions. Likely, user-space programs typically rely on standard libraries (e.g. <em>malloc()</em> in C, or memory allocators such as jemalloc or tcmalloc) that request and manage chunks of memory via these system calls. As mentioned, high-level languages such as Python or Java implement their own garbage collection and runtime memory models, relying heavily on the OS to provide underlying memory primitives.</p>

<!-- a data buffer (or just buffer) is a region of memory used to store data temporarily while it is being moved from one place to another. -->

<h3 id="33-file-management"><strong>3.3. File Management</strong></h3>

<p style="margin-bottom: 12px;"> </p>

<p>A file is a named, persistent sequence of bytes managed by the operating system and stored on physical or virtual media. It serves as an abstraction over raw storage, enabling programs to access data through a consistent interface. Although the OS treats a file as a linear byte stream with no intrinsic meaning, its extension‚Äîsuch as .txt for plain text or .mp3 for audio‚Äîoften signals the format and intended interpretation of its contents. In practice, the file extension acts as a contract between applications and data, guiding how information is parsed, rendered, or executed.</p>

<p>The data within a file may follow arbitrary formats depending on application needs: (i) structured (e.g. CSV, Parquet), (ii) semi-structured (e.g. JSON, XML), or (iii) binary (e.g. compiled code, images, or model checkpoints). In general-purpose computing, files may store text documents, scripts, or executables. In contrast, fields like machine learning and data engineering rely on file formats that embed schema or use compact binary layouts for performance. Examples include .h5 (HDF5), .parquet, and .arrow, which are designed for efficient, structured access and parallel processing at scale.</p>

<p>The OS is responsible for file creation, access, naming, storage allocation, and metadata management. It exposes a hierarchical file system that maps human-readable paths (e.g. /home/user/data.csv) to physical data blocks. Key abstractions like inodes (in Unix-like systems) store metadata including file size, timestamps, ownership, and pointers to data blocks. The OS also enforces access control via permissions and user/group ownership and supports special files‚Äîsuch as symbolic links, device nodes, sockets, and FIFOs‚Äîthat extend file semantics to hardware interfaces and inter-process communication.</p>

<p>Interaction with files is mediated through file descriptors, small integers that index open file entries in a process-specific table managed by the kernel. Core file operations‚Äîopen(), read(), write(), and close()‚Äîform the low-level interface exposed via system calls. For more advanced I/O, the mmap() system call allows files to be mapped directly into virtual memory, enabling efficient pointer-based access and inter-process sharing. Behind the scenes, the OS manages caches (e.g. page cache and buffer cache) to reduce disk latency and supports direct I/O or asynchronous I/O for performance-critical workloads.</p>

<p>Several advanced mechanisms underpin modern file systems. Mounting attaches a file system (e.g. a USB stick or network volume) to a specific directory path within the global namespace. Journaling, used in systems like ext4 or XFS, logs metadata updates to support crash recovery. Advanced file systems such as ZFS and Btrfs offer features like copy-on-write, snapshots, checksumming, and compression for enhanced reliability. File locking (via flock() or fcntl()) helps avoid race conditions but can introduce deadlocks if not managed carefully. Virtual file systems like /proc and /sys expose live kernel and hardware state through file-like interfaces, enabling introspection tools to query memory usage, CPU load, or device health.</p>

<p>In ML and LLM workflows, file handling is central to system performance and model scalability. Datasets are often stored in binary formats such as TFRecord, HDF5, or Apache Arrow, which support partial reads and parallel access across workers. PyTorch‚Äôs DataLoader leverages multiprocessing to fetch data from shared disks or cloud object stores mounted via FUSE drivers like s3fs. For inference, quantized model formats such as GPT-generated Unified Format (.gguf) or .pth are optimized for memory alignment, minimal I/O overhead, and streaming. Tools like llama.cpp use memory mapping or chunked loading to reduce memory footprint and accelerate loading. In distributed or multi-node environments, consistency and throughput are maintained via parallel file systems (e.g. Lustre) or eventual-consistency object stores (e.g. Amazon S3), which must support atomic writes and concurrent access to training logs, checkpoints, and model artifacts.</p>

<!-- https://www.cs.miami.edu/home/visser/Courses/CSC322-09S/Content/UNIXUse/FileSystem.shtml -->
<!-- https://www.reddit.com/r/linux/comments/qkm01c/a_refresher_on_the_linux_file_system_structure/ -->

<!-- ### **3.4. Device Management**
<p style="margin-bottom: 12px;"> </p>

- Device drivers: derive definition, NVIDIA CUDA drivers, AMD ROCm, or TPU runtime drivers allow the OS to interface with ML accelerators.
- I/O scheduling: Efficient I/O scheduling is vital for high-throughput data pipelines and sharded dataset loading -->

				
					<br>
<hr>
<br>
<div class="comment">
	<p>I gathered words solely for my own purposes without any intention to break the rigorosity of the subjects.<br>
	Well, I prefer eating corn in spiral <i class="fa fa-cutlery"></i>.</p>
</div>
				
			</div>
		</div>
    
	<script src="/assets/js/001.js"></script>
	<script src="/assets/js/002.js"></script>
	<div class="footer">
		<p>Code licensed under <a href="https://github.com/h01000110/h01000110.github.io/blob/master/LICENSE" target="_blank">MIT License</a></p>
	</div>
</body>
</html>