<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
	<link rel="stylesheet" href="/assets/css/atom-one-light.css">
    
        <title>301. ml literature</title>
		<link rel="stylesheet" type="text/css" href="/assets/css/002.css">
    
	<link rel="stylesheet" href="/assets/css/font-awesome.min.css">
	<link rel="shortcut icon" href="/assets/img/favicon.ico" type="image/x-icon">
	<link rel="icon" href="/assets/img/favicon.ico" type="image/x-icon">
	<script src="/assets/js/highlight.pack.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>
	
		<!--http://benlansdell.github.io/computing/mathjax/-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    extensions: [
      "tex2jax.js",
      "MathMenu.js",
      "MathZoom.js",
      "AssistiveMML.js",
      "a11y/accessibility-menu.js"
    ],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    jax: ["input/TeX", "output/CommonHTML"],
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
</script>
<!-- You may add the following into MathJax.Hub.Config:
CommonHTML: {
    scale: 85
}-->
<script type="text/javascript" id="MathJax-script" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
	
</head>
<body>
	<div class="wrapper">
		<div class="default_title">
			<img src="/assets/img/mycomputer.png" />
			
				<h1>Hikikomori</h1>
			
		</div>
		<ul class="topbar">
	<a href="/me"><li><u>A</u>bout</li></a>
	<a href="https://www.linkedin.com/in/sung-kim-28b78a7b/" target="_blank"><li><u>L</u>inkedIn</li></a>
	<!-- 
		<a href="http://soundcloud.com/h01000110" target="_blank"><li><u>S</u>oundcloud</li></a>
	-->
</ul>
		<div class="tag_list">
			<ul id="tag-list">
				<li><a href="/" ><img src="/assets/img/disk.png" />(C:)</a>
			<ul>
				
				
				<li><a href="/tag/cs300/" title="cs300"><img src="/assets/img/folder.ico" />cs300</a></li>
				
				<li><a href="/tag/cs400/" title="cs400"><img src="/assets/img/folder.ico" />cs400</a></li>
				
				<li><a href="/tag/math200/" title="math200"><img src="/assets/img/folder.ico" />math200</a></li>
				
				<li><a href="/tag/math500/" title="math500"><img src="/assets/img/folder.ico" />math500</a></li>
				
			</ul>
				</li>
			</ul>
		</div>
		<div class="post_list">
			
				<ul>
					
					<li><a href="/20240201/functional-analysis" title="501. functional analysis"><img src="/assets/img/file.ico" title="501. functional analysis" />501. functional analysis</a></li>
					
					<li><a href="/20240105/parallel-computing" title="Parallel Computing"><img src="/assets/img/file.ico" title="Parallel Computing" />Parallel Computing</a></li>
					
					<li><a href="/20240104/networking" title="Networking"><img src="/assets/img/file.ico" title="Networking" />Networking</a></li>
					
					<li><a href="/20240103/virtualisation" title="Virtualisation"><img src="/assets/img/file.ico" title="Virtualisation" />Virtualisation</a></li>
					
					<li><a href="/20240102/tmp" title="402. operating system"><img src="/assets/img/file.ico" title="402. operating system" />402. operating system</a></li>
					
					<li><a href="/20240102/operating-system" title="402. operating system"><img src="/assets/img/file.ico" title="402. operating system" />402. operating system</a></li>
					
					<li><a href="/20240101/computer" title="401. dl compilation"><img src="/assets/img/file.ico" title="401. dl compilation" />401. dl compilation</a></li>
					
					<li><a href="/20230101/ml-paper" title="301. ml literature"><img src="/assets/img/file.ico" title="301. ml literature" />301. ml literature</a></li>
					
					<li><a href="/20220112/markov-chain" title="212. markov chain"><img src="/assets/img/file.ico" title="212. markov chain" />212. markov chain</a></li>
					
					<li><a href="/20220111/martingale" title="211. martingale"><img src="/assets/img/file.ico" title="211. martingale" />211. martingale</a></li>
					
					<li><a href="/20220110/stochastic-process" title="210. stochastic process"><img src="/assets/img/file.ico" title="210. stochastic process" />210. stochastic process</a></li>
					
					<li><a href="/20220109/laws-of-the-iterated-logarithm" title="209. law of the iterated logarithm"><img src="/assets/img/file.ico" title="209. law of the iterated logarithm" />209. law of the iterated logarithm</a></li>
					
					<li><a href="/20220108/central-limit-theorem" title="208. central limit theorem"><img src="/assets/img/file.ico" title="208. central limit theorem" />208. central limit theorem</a></li>
					
					<li><a href="/20220107/laws-of-large-number" title="207. law of large number"><img src="/assets/img/file.ico" title="207. law of large number" />207. law of large number</a></li>
					
					<li><a href="/20220106/modes-of-convergence" title="206. mode of convergence"><img src="/assets/img/file.ico" title="206. mode of convergence" />206. mode of convergence</a></li>
					
					<li><a href="/20220105/characteristic-function" title="205. characteristic function"><img src="/assets/img/file.ico" title="205. characteristic function" />205. characteristic function</a></li>
					
					<li><a href="/20220104/inequality" title="204. inequality"><img src="/assets/img/file.ico" title="204. inequality" />204. inequality</a></li>
					
					<li><a href="/20220103/expectation" title="203. expectated value"><img src="/assets/img/file.ico" title="203. expectated value" />203. expectated value</a></li>
					
					<li><a href="/20220102/random-variable" title="202. random variable"><img src="/assets/img/file.ico" title="202. random variable" />202. random variable</a></li>
					
					<li><a href="/20220101/probability" title="201. probability theory"><img src="/assets/img/file.ico" title="201. probability theory" />201. probability theory</a></li>
					
				</ul>
			
		</div>
		<div class="post_total">
			
				<div class="left">20 object(s)</div>
			
			<div class="right">&nbsp;</div>
		</div>
	</div>
	
        <div class="content">
			<div class="post_title">
				<img src="/assets/img/file.png" />
				<h1>301. ml literature</h1>
				<a href="/"><div class="btn"><span class="fa fa-times"></span></div></a>
				<div class="btn btn_max"><span class="fa fa-window-maximize"></span></div>
				<div class="btn"><span class="fa fa-window-minimize"></span></div>
			</div>
			<ul class="topbar">
				<li>January 1, 2023</li>
			</ul>
			<div class="post_content">
        		<h2 id="i-list-of-conferences">I. List of Conferences</h2>
<hr />
<p>More will be added if necessary.
$\\$</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left"><strong>Conference</strong></th>
      <th style="text-align: left"><strong>Alias</strong></th>
      <th style="text-align: center"><strong>Founded</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Association for the Advancement of Artificial Intelligence</td>
      <td style="text-align: left"><a href="">AAAI</a></td>
      <td style="text-align: center">1980</td>
    </tr>
    <tr>
      <td style="text-align: left">Association for Computational Linguistics</td>
      <td style="text-align: left"><a href="">ACL</a></td>
      <td style="text-align: center">2013</td>
    </tr>
    <tr>
      <td style="text-align: left">IEEE Conference on Computer Vision and Pattern Recognition</td>
      <td style="text-align: left"><a href="">CVPR</a></td>
      <td style="text-align: center">1983</td>
    </tr>
    <tr>
      <td style="text-align: left">Conference on Empirical Methods in Natural Language Processing</td>
      <td style="text-align: left"><a href="">EMNLP</a></td>
      <td style="text-align: center">1987</td>
    </tr>
    <tr>
      <td style="text-align: left">International Conference on Learning Representations</td>
      <td style="text-align: left"><a href="">ICLR</a></td>
      <td style="text-align: center">1962</td>
    </tr>
    <tr>
      <td style="text-align: left">International Conference on Machine Learning</td>
      <td style="text-align: left"><a href="">ICML</a></td>
      <td style="text-align: center">1996</td>
    </tr>
    <tr>
      <td style="text-align: left">Institute of Electrical and Electronics Engineers</td>
      <td style="text-align: left"><a href="">IEEE</a></td>
      <td style="text-align: center">1963</td>
    </tr>
    <tr>
      <td style="text-align: left">International Joint Conference on Artificial Intelligence</td>
      <td style="text-align: left"><a href="">IJCAI</a></td>
      <td style="text-align: center">1969</td>
    </tr>
    <tr>
      <td style="text-align: left">Conference and Workshop on Neural Information Processing Systems</td>
      <td style="text-align: left"><a href="">NeurIPS</a></td>
      <td style="text-align: center">1982</td>
    </tr>
    <tr>
      <td style="text-align: left">ACM Conference on Recommender Systems</td>
      <td style="text-align: left"><a href="">RecSys</a></td>
      <td style="text-align: center">2007</td>
    </tr>
  </tbody>
</table>

<p>$\\$</p>

<h2 id="ii-list-of-papers">II. List of Papers</h2>
<hr />
<p>Those should be organised better.</p>

<p><strong>_arXiv</strong></p>

<pre><code>Harnessing the Universal Geometry of Embeddings, 2025
"DeepSeek-R1": Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, 2025
Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling, 2025
Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters, 2024
"Mixtral": Mixtral of Experts (8x7B), 2024
SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills, 2023
A Survey of Large Language Models, 2023
Efficient Memory Management for Large Language Model Serving with PagedAttention, 2023
"Mamba": Linear-Time Sequence Modeling with Selective State Spaces, 2023
"LLaMA": Open and Efficient Foundation Language Models, 2023
"Chinchilla": Training Compute-Optimal Large Language Models, 2022
FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness, 2022
LoRA: Low-Rank Adaptation of Large Language Models, 2021
"Dall-E": Zero-Shot Text-to-Image Generation, 2021
"T5": Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, 2019
"RoBERTa": A Robustly Optimized BERT Pretraining Approach, 2019
"GPT-2": Language Models are Unsupervised Multitask Learners, 2018
Representation Learning with Contrastive Predictive Coding, 2018
Deep Learning based Recommender System: A Survey and New Perspectives, 2017
</code></pre>

<p><strong>AAAI</strong></p>

<pre><code>Vision Transformers are Robust Learners, 2022
Multi-Task Learning of Knowledge Tracing and Option Tracing for Better Student Assessment, 2022
</code></pre>

<p><strong>ACL</strong></p>

<pre><code>Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks, 2021
"BART": Denoising Seq2Seq Pre-training for NLG, Translation, and Comprehension, 2020
Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned, 2019
What Does BERT Look At? An Analysis of BERT`s Attention, 2019
Self-Attention with Relative Position Representations, 2018
Universal Language Model Fine-tuning for Text Classification, 2018
"Transformer-XL": Attentive Language Models Beyond a Fixed-Length Context, 2018
"BERT": Pre-training of Deep Bidirectional Transformers for Language Understanding, 2018
Improving Neural Network Translation Models with Multiple Subword Candidates, 2018
Neural Machine Translation of Rare Words with Subword Units, 2016
</code></pre>

<p><strong>CVPR</strong></p>

<pre><code>"CLIP": Learning Transferable Visual Models From Natural Language Supervision, 2021
FaceNet: A Unified Embedding for Face Recognition and Clustering, 2015
</code></pre>

<p><strong>EMNLP</strong></p>

<pre><code>Text Embeddings Reveal (Almost) As Much As Text, 2023
GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints, 2023
"SimCSE": Simple Contrastive Learning of Sentence Embeddings, 2021
Dense Passage Retrieval for Open-Domain Question Answering, 2020
BERT-flow: On the Sentence Embeddings from Pre-trained Language Models, 2020
"Sentence-BERT": Sentence Embeddings using Siamese BERT-Networks, 2019
How Contextual are Contextualized Word Representations?, 2019
Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks, 2019
A simple and language independent subword tokenizer and detokenizer for Neural Text Processing, 2018
Effective Approaches to Attention-based Neural Machine Translation, 2015
</code></pre>

<p><strong>ICLR</strong></p>

<pre><code>Demystifying Embedding Spaces using Large Language Models, 2024
"FLAN": Finetuned Language Models are Zero-Shot Learners, 2022
"BEiT": Bert Pre-Training of Image Transfomers, 2022
"ViT": An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, 20204. 
"Performer": Rethinking Attention with Performers, 2020
Decoupled Weight Decay Regularization, 2019
Don`t Decay the Learning Rate, Increase the Batch Size, 2018
On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima, 2017
Neural Machine Translation by Jointly Learning to Align and Translate, 2015
</code></pre>

<p><strong>ICML</strong></p>

<pre><code>Fast Inference from Transformers via Speculative Decoding, 2023
"SimCLR": A Simple Framework for Contrastive Learning of Visual Representations, 2020
Understanding Contrastive Learning through Alignment and Uniformity on the Hypersphere, 2020
On Layer Normalization in the Transformer Architecture, 2020
Towards A Unified Analysis of Random Fourier Features, 2019
</code></pre>

<p><strong>IEEE</strong></p>

<pre><code>"LiT": Zero-Shot Transfer with Locked-image text Tuning, 2022
"Swin Transformer": Hierarchical Vision Transformer using Shifted Windows, 2021
Random Features for Kernel Approximation: A survey on Algorithms, Theory, and Beyond, 2020
"Faiss": Billion-scale similarity search with GPUs, 2017
Efficient and robust approximate nearest neighbor search using HNSW graphs, 2016
Optimized Product Quantization, 2013
Product Quantization for Nearest Neighbor Search, 2011
Factorization Machines, 2010
Collaborative Filtering for Implicit Feedback Datasets, 2008
</code></pre>

<p><strong>IJCAI</strong></p>

<pre><code>"DeepFM": A Factorization-Machine based Neural Network for CTR Prediction, 2016
</code></pre>

<p><strong>NeurIPS</strong></p>

<pre><code>The Impact of Positional Encoding on Length Generalization in Transformers, 2023
QLoRA: Efficient Finetuning of Quantized LLMs, 2023
LLM.int8: 8-bit Matrix Multiplication for Transformers at Scale, 2022
Training language models to follow instructions with human feedback, 2022
Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, 2022
"RAG": Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, 2020
"GPT-3": Language Models are Few-Shot Learners, 2020
"XLNet": Generalized Autoregressive Pretraining for Language Understanding, 2019
When does Label Smoothing Help?, 2019
Visualizing the Loss Landscape of Neural Nets, 2018
How Does Batch Normalization Help Optimization?, 2018
Train longer, generalize better: closing the generalization gap in large batch training of NNs, 2017
"Transformer": Attention is All you Need, 2017
</code></pre>

<p><strong>RecSys</strong></p>

<pre><code>Logistic Matrix Factorization for Implicit Feedback Data, 2020
Deep Neural Networks for YouTube Recommendations, 2016
Wide &amp; Deep Learning for Recommender System, 2016
Bayesian Personalized Ranking from Implicit Feedback, 2012
The YouTube video recommendation system, 2010
Matrix Factorization Technique for Recommender Systems, 2009
</code></pre>

				
					<br>
<hr>
<br>
<div class="comment">
	<p>I gathered words solely for my own purposes without any intention to break the rigorosity of the subjects.<br>
	Well, I prefer eating corn in spiral <i class="fa fa-cutlery"></i>.</p>
</div>
				
			</div>
		</div>
    
	<script src="/assets/js/001.js"></script>
	<script src="/assets/js/002.js"></script>
	<div class="footer">
		<p>Code licensed under <a href="https://github.com/h01000110/h01000110.github.io/blob/master/LICENSE" target="_blank">MIT License</a></p>
	</div>
</body>
</html>