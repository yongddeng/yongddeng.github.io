<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
	<link rel="stylesheet" href="/assets/css/atom-one-light.css">
    
        <title>204. inequality</title>
		<link rel="stylesheet" type="text/css" href="/assets/css/002.css">
    
	<link rel="stylesheet" href="/assets/css/font-awesome.min.css">
	<link rel="shortcut icon" href="/assets/img/favicon.ico" type="image/x-icon">
	<link rel="icon" href="/assets/img/favicon.ico" type="image/x-icon">
	<script src="/assets/js/highlight.pack.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>
	
		<!--http://benlansdell.github.io/computing/mathjax/-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    extensions: [
      "tex2jax.js",
      "MathMenu.js",
      "MathZoom.js",
      "AssistiveMML.js",
      "a11y/accessibility-menu.js"
    ],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    jax: ["input/TeX", "output/CommonHTML"],
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
</script>
<!-- You may add the following into MathJax.Hub.Config:
CommonHTML: {
    scale: 85
}-->
<script type="text/javascript" id="MathJax-script" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
	
</head>
<body>
	<div class="wrapper">
		<div class="default_title">
			<img src="/assets/img/mycomputer.png" />
			
				<h1>Hikikomori</h1>
			
		</div>
		<ul class="topbar">
	<a href="/me"><li><u>A</u>bout</li></a>
	<a href="https://www.linkedin.com/in/sung-kim-28b78a7b/" target="_blank"><li><u>L</u>inkedIn</li></a>
	<!-- 
		<a href="http://soundcloud.com/h01000110" target="_blank"><li><u>S</u>oundcloud</li></a>
	-->
</ul>
		<div class="tag_list">
			<ul id="tag-list">
				<li><a href="/" ><img src="/assets/img/disk.png" />(C:)</a>
			<ul>
				
				
				<li><a href="/tag/cs300/" title="cs300"><img src="/assets/img/folder.ico" />cs300</a></li>
				
				<li><a href="/tag/cs400/" title="cs400"><img src="/assets/img/folder.ico" />cs400</a></li>
				
				<li><a href="/tag/math200/" title="math200"><img src="/assets/img/folder.ico" />math200</a></li>
				
				<li><a href="/tag/math500/" title="math500"><img src="/assets/img/folder.ico" />math500</a></li>
				
			</ul>
				</li>
			</ul>
		</div>
		<div class="post_list">
			
				<ul>
					
					<li><a href="/20240201/functional-analysis" title="501. functional analysis"><img src="/assets/img/file.ico" title="501. functional analysis" />501. functional analysis</a></li>
					
					<li><a href="/20240105/parallel-computing" title="Parallel Computing"><img src="/assets/img/file.ico" title="Parallel Computing" />Parallel Computing</a></li>
					
					<li><a href="/20240104/networking" title="Networking"><img src="/assets/img/file.ico" title="Networking" />Networking</a></li>
					
					<li><a href="/20240103/virtualisation" title="Virtualisation"><img src="/assets/img/file.ico" title="Virtualisation" />Virtualisation</a></li>
					
					<li><a href="/20240102/tmp" title="402. operating system"><img src="/assets/img/file.ico" title="402. operating system" />402. operating system</a></li>
					
					<li><a href="/20240102/operating-system" title="402. operating system"><img src="/assets/img/file.ico" title="402. operating system" />402. operating system</a></li>
					
					<li><a href="/20240101/computer" title="401. dl compilation"><img src="/assets/img/file.ico" title="401. dl compilation" />401. dl compilation</a></li>
					
					<li><a href="/20230101/ml-paper" title="301. ml literature"><img src="/assets/img/file.ico" title="301. ml literature" />301. ml literature</a></li>
					
					<li><a href="/20220112/markov-chain" title="212. markov chain"><img src="/assets/img/file.ico" title="212. markov chain" />212. markov chain</a></li>
					
					<li><a href="/20220111/martingale" title="211. martingale"><img src="/assets/img/file.ico" title="211. martingale" />211. martingale</a></li>
					
					<li><a href="/20220110/stochastic-process" title="210. stochastic process"><img src="/assets/img/file.ico" title="210. stochastic process" />210. stochastic process</a></li>
					
					<li><a href="/20220109/laws-of-the-iterated-logarithm" title="209. law of the iterated logarithm"><img src="/assets/img/file.ico" title="209. law of the iterated logarithm" />209. law of the iterated logarithm</a></li>
					
					<li><a href="/20220108/central-limit-theorem" title="208. central limit theorem"><img src="/assets/img/file.ico" title="208. central limit theorem" />208. central limit theorem</a></li>
					
					<li><a href="/20220107/laws-of-large-number" title="207. law of large number"><img src="/assets/img/file.ico" title="207. law of large number" />207. law of large number</a></li>
					
					<li><a href="/20220106/modes-of-convergence" title="206. mode of convergence"><img src="/assets/img/file.ico" title="206. mode of convergence" />206. mode of convergence</a></li>
					
					<li><a href="/20220105/characteristic-function" title="205. characteristic function"><img src="/assets/img/file.ico" title="205. characteristic function" />205. characteristic function</a></li>
					
					<li><a href="/20220104/inequality" title="204. inequality"><img src="/assets/img/file.ico" title="204. inequality" />204. inequality</a></li>
					
					<li><a href="/20220103/expectation" title="203. expectated value"><img src="/assets/img/file.ico" title="203. expectated value" />203. expectated value</a></li>
					
					<li><a href="/20220102/random-variable" title="202. random variable"><img src="/assets/img/file.ico" title="202. random variable" />202. random variable</a></li>
					
					<li><a href="/20220101/probability" title="201. probability theory"><img src="/assets/img/file.ico" title="201. probability theory" />201. probability theory</a></li>
					
				</ul>
			
		</div>
		<div class="post_total">
			
				<div class="left">20 object(s)</div>
			
			<div class="right">&nbsp;</div>
		</div>
	</div>
	
        <div class="content">
			<div class="post_title">
				<img src="/assets/img/file.png" />
				<h1>204. inequality</h1>
				<a href="/"><div class="btn"><span class="fa fa-times"></span></div></a>
				<div class="btn btn_max"><span class="fa fa-window-maximize"></span></div>
				<div class="btn"><span class="fa fa-window-minimize"></span></div>
			</div>
			<ul class="topbar">
				<li>January 4, 2022</li>
			</ul>
			<div class="post_content">
        		<h1 id="inequality">Inequality</h1>
<hr />
<p>Queried: Who invented inequalities; Google responded: The signs for greater than and less than were introduced by Thomas Harriot (1560-1621). He initially wrote triangular symbols that the editor of his book altered to what resemble the modern symbols. Inequalities are essential for the subject in order to adapt some mathematics originated in geometry and linear algebra.</p>

<h2 id="i">I</h2>
<hr />
<p>Suppose $X:\Omega \to \mathbb{R}$ is a random variable and $g:\mathbb{R} \to [0,\infty]$ is a measurable function. If we let $g_{\ast}(A) = \inf \lbrace g(y): y \in A \rbrace$ for any $A \in \mathcal{B}_{\mathbb{R}}$, then <a href="https://en.wikipedia.org/wiki/Markov%27s_inequality">Markov’s inequality</a> guarantees that $g_{\ast}(A)P(X \in A) \leq \operatorname{E}g(X)I_{X \in A} \leq \operatorname{E}g(X)$ with $P(X \in A) = \operatorname{E}I_{X \in A}$. Moreover, if $g(x) = x^+$ and $A = [a,\infty)$ for some $a &gt; 0$, then $P(X \geq a) \leq a^{-1}\operatorname{E}X^+$. The result is particularly appealing for some $X \geq 0$. Whereas, if $g(x) = {\vert x \vert}^q$ and $A = (-\infty, a] \cup [a, \infty)$ for some $a &gt; 0$, then $P(\vert X \vert \geq a) \leq a^{-q}\operatorname{E}{\vert X \vert}^q$. A tighter upper bound $P(\vert Y - \operatorname{E}Y \vert \geq a) \leq a^{-2}\operatorname{Var}Y$, so-called <a href="https://en.wikipedia.org/wiki/Chebyshev%27s_inequality">Chebyshev’s inequality</a> <strong>(#1)</strong>, can also be constructed by letting $q=2$.</p>

<p>The second instance also works for a sequence $(X_n)_{n\in\mathbb{N}}$ consists of ind. $X_n$ such that $\operatorname{E}X_n = 0$ and $\operatorname{Var}X_n &lt; \infty$. Specifically, $P(\vert S_n - \operatorname{E}S_n \vert \geq a) \leq a^{-2}\operatorname{Var}S_n$ for $S_n = \Sigma_{k=1}^n X_k$. If $(X_n)_{n\in\mathbb{N}}$ are i.i.d., then $P(\vert S_n - \operatorname{E}S_n \vert \geq a) \leq a^{-2} n\operatorname{Var}X_1$. We may use <a href="">Truncated Chebyshev’s inequality</a> without finite variances <strong>(#2)</strong>. That is, given $\tilde{X}_n = X_n - \operatorname{E}X_n$ and $X_n \in L^2$, we compute $P(\vert \tilde{X}_n \vert \geq a) \leq a^{-2}\operatorname{Var}\tilde{X}_n$. Moreover, if $\tilde{S}_n = \Sigma_{k=1}^n \tilde{X}_k$, then $P(\vert S - \operatorname{E}\tilde{S}_n \vert \geq a) \leq a^{-2} \Sigma_{k=1}^n\operatorname{Var}\tilde{X}_k + \Sigma_{k=1}^{n} P(\vert X_k \vert &gt; b_k)$, where $\tilde{X}_k = X_k$ whenever $\vert X_k \vert \leq b_k$ and $\tilde{X}_k = c$ otherwise. We can assume that $(b_k)_{k\in\mathbb{N}}$ is a seq. of positive real numbers wlog.</p>

<p>Chebyshev’s inequality is extended to <a href="https://en.wikipedia.org/wiki/Kolmogorov%27s_inequality">Kolmogorov’s inequality</a> (i.e. maximal inequality) $P(\max_{1 \leq k\leq n} \vert S_k \vert \geq a) \leq a^{-2}\operatorname{Var}S_n$ which we may restate by $P(\max_{n \leq k\leq m} \vert S_k \vert \geq a) \leq a^{-2}\operatorname{Var}S_m$. Its convenience arises when we need to bound the worst case deviation (i.e. it can occure at any $k$) of a partial sum process $(S_n)_{n\in\mathbb{N}}$ consists of $S_{n} = \Sigma_{k=1}^{n}X_k$, where each $X_k$ are pairwise ind.. It can be generalised to <a href="https://www.jstor.org/stable/25049653">Hájek–Rényi inequality</a> assuring that $P(\max_{1 \leq k \leq n} d_k \vert S_k \vert \geq a) \leq a^{-2}\Sigma_{k=1}^{n} d_{k}^{2}\operatorname{Var}X_k$, where $(d_k)_{1 \leq k\leq n}$ are some non-increasing positive real numbers. That is, whenever $d_k =1$ for all $k$, Hájek–Rényi inequality returns to Kolmogorov’s inequality.</p>

<h2 id="ii">II</h2>
<hr />
<p>Some inequalities rather relate moments of sums to sum of moments. Suppose $X$ and $Y$ both lie on the same probability space while $\operatorname{E}{\vert X \vert}^r, \operatorname{E}{\vert Y \vert}^r &lt; \infty$ for some $r &gt; 0$. The <a href="https://jeanmariedufour.github.io/ResE/Dufour_2008_C_TS_Moments.pdf">$c_r$-inequality</a> assures that $\operatorname{E}{\vert X+Y \vert}^r \leq c_r(\operatorname{E}{\vert X \vert}^r + \operatorname{E}{\vert Y \vert}^r)$, where $c_r = 1$ if $r \leq 1$ and $c_r = 2^{r-1}$ if $r \geq 1$. In fact, we have the <a href="https://en.wikipedia.org/wiki/Triangle_inequality">triangle inequality</a> of the $r$-th order $\operatorname{E}{\vert X+Y \vert}^r \leq (\operatorname{E}\vert X \vert + \operatorname{E}\vert Y \vert)^r \leq \operatorname{E} {\vert X \vert}^r + \operatorname{E}{\vert Y \vert}^r$ for all $r \in (0,1]$. These show that a sum of random variables are integrable if all summands are integrable. By using the $c_r$-inequality, we can prove that the converse holds for all $r &gt; 0$ if and only if $X$ and $Y$ are independent.</p>

<p><a href="https://en.wikipedia.org/wiki/Minkowski_inequality">Minkowski’s inequality</a> instead concerns sums of random variables in an $L^p$-space using the <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)">$L^p$-norm</a> ${\left\lVert \cdot \right\rVert}_p = (\operatorname{E} {\vert \cdot \vert}^p)^{1/p}$. Namely, if $\operatorname{E}{\vert X \vert}^p, \operatorname{E}{\vert Y \vert}^p &lt; \infty$ for any $p \geq 1$, then ${\left\lVert X+Y \right\rVert}_p \leq {\left\lVert X \right\rVert}_p + {\left\lVert Y \right\rVert}_p$. This becomes to the usual Euclidean norm for $p=2$, and encompoasses the triangle inequality and extends it $L^{p}$-norm for $p \geq 1$. Also, since the operator $\left\lVert \cdot \right\rVert: \mathbb{R} \to [0,\infty)$ is linear, ${\left\lVert \alpha{X} + \beta{Y} \right\rVert}_p = \vert \alpha \vert {\left\lVert X \right\rVert}_p + \vert \beta \vert {\left\lVert Y \right\rVert}_p$ for all $\alpha, \beta \in \mathbb{R}$ and ${\left\lVert X \right\rVert}_p, {\left\lVert Y \right\rVert}_p \in L^p$. Note that an $L^p$-space is indeed a vector space <strong>(#3)</strong>, so is a Banach space if all sequence is a Cauchy sequence, and a Hilbert space if we futher have a covariance (i.e. an inner product).</p>

<p><a href="https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality">Hölder’s inequality</a> on the contrary concerns products of random variables in an $L^p$-space. That is, if $p, q &gt; 1$ and $p^{-1} + q^{-1} = 1$ <strong>(#4)</strong>, then $\vert\langle X,Y \rangle\vert := {\vert \operatorname{E}XY \vert} \leq {\operatorname{E}\vert XY \vert} \leq {\left\lVert X \right\rVert}_p \cdot {\left\lVert Y \right\rVert}_q$. The second inequality makes sense once we observe that $\operatorname{E}({\vert X \vert}/{\left\lVert X \right\rVert}_p \cdot {\vert Y \vert}/{\left\lVert Y \right\rVert}_p) \leq p^{-1}\operatorname{E}({\vert X \vert}^p/{\left\lVert X \right\rVert}_p^p) + q^{-1}\operatorname{E}({\vert Y \vert}^q/{\left\lVert Y \right\rVert}_q^q) = p^{-1} + q^{-1}$. If we suppose $p=q=2$, then <a href="">Cauchy-Schwarz ineqaulity</a> bounds $\operatorname{E}{\vert XY \vert} \leq {\left\lVert X \right\rVert}_2 \cdot {\left\lVert Y \right\rVert}_2$. Consequently, ${\vert \operatorname{Cov}(X,Y) \vert} \leq \sqrt{\operatorname{Var}X \cdot \operatorname{Var}Y}$ and also ${\vert \rho_{X,Y} \vert} \leq 1$, where $\rho_{X,Y} = \operatorname{Cov}(X,Y) / \sqrt{\operatorname{Var}X \cdot \operatorname{Var}Y}$ is the measure of statistics so-called <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Pearson correlation coefficient</a>.</p>

<h2 id="iii">III</h2>
<hr />
<p><a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">Jensen’s inequality</a> in general states that a secant line as a weighted (i.e. total weight of $1$) means of a convex function $\varphi$ is drawn over $\varphi$. Whereas, given a probability space, an $\mathbb{S}$-valued integrable random variable $X$, and a measurable convex function $\varphi:\mathbb{R}\to\mathbb{R}$, it assures that $\varphi(\operatorname{E}(X\vert\mathcal{G})) \leq \operatorname{E}(\varphi(X)\vert\mathcal{G})$, where $\mathcal{G} \subset \mathcal{F}$ is a sub-$\sigma$-algebra. If a set $\mathbb{S}$ is given to be the real axis $\mathbb{R}$ and $\mathcal{G}$ is the trivial σ-algebra only containing $\lbrace \Omega, \emptyset \rbrace$, then $\varphi(\operatorname{E}X) \leq \operatorname{E}\varphi(X)$ and the equality holds if and only if $\varphi$ is linear on a <a href="https://en.wikipedia.org/wiki/Convex_set">convex set</a> $A \subseteq \mathbb{R}$ such that $P(X \in A) = 1$ <strong>(#5)</strong>. Jensen’s inequality and Cauchy-Schwarz inequality have a wide range of uses.</p>

<p>The direct use of Jensen’s inequality on a density $f$ is given by $\varphi(\int_{\mathbb{R}} g(x)f(x)\,\mathrm{d}x) \leq \int_{\mathbb{R}} \varphi(g(x))f(x)\,\mathrm{d}x$, where $g$ is Borel measurable (i.e. $g(x) = x$). If $\varphi(x) = x^{2n}$ for some $n \in \mathbb{N}$, then $\varphi^{\prime\prime}(x) \geq 0$ for all $x\in\mathbb{R}$ and $\varphi(\operatorname{E}X) = (\operatorname{E}X)^{2n} \leq \operatorname{E}X^{2n}$ which yields $\operatorname{Var}X = \operatorname{E}X^2 - (\operatorname{E}X)^2 \geq 0$. Moreover, if $p$ and $q$ are the true and another density of $X$, and we set $Y(x) = q(x)/p(x)$ while $\varphi(y) = -\log(y)$ <strong>(#6)</strong>, then the construction yields <a href="https://en.wikipedia.org/wiki/Gibbs%27_inequality">Gibbs’ inequality</a> $-\Sigma_{x}p(x)\log{p(x)} \leq -\Sigma_{x}p(x)\log{q(x)}$, which then relates to the non-negativity of the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL-Divergence</a> $D_{KL}(P \vert\vert Q) = -\Sigma_{x}p(x) \log{[q(x)/p(x)]} = \Sigma_{x}p(x) \log{[p(x)/q(x)]} \geq 0$.</p>

<p>We can geometrically work with $X \in H$, as if they were vectors in Euclidean space, if proper notions of length and orthogonality are held in a set $H = \lbrace X: \operatorname{E}X^2 &lt; \infty \rbrace$. For example, given $X,Y \in H$, the OLS of $Y$ can be done by finding the best linear function of $X$. Namely, $L(Y \vert X) = \operatorname{E}Y + (X-\operatorname{E}X) \cdot {\operatorname{Cov}(X,Y) \over \operatorname{Var}X}$ is the best function which uniquely satisfies both (i) $\operatorname{E}[L(Y \vert X)] = EY$; (ii) $\operatorname{Cov}[X, L(Y \vert X)] = \operatorname{Cov}(X,Y)$; If $U \in H$ is any linear function of $X$, then equality holds for $\operatorname{E}[Y - L(Y \vert X)]^2 \leq \operatorname{E}(Y-U)^2$ if and only if $U = L(Y \vert X)$. Otherwise, the mean squared error will be presented by $\operatorname{E}[Y - L(Y \vert X)]^2 = \operatorname{Var}(Y)[1-\operatorname{Cor}^2(X,Y)]$.</p>

<h2 id="iv">IV</h2>
<hr />
<p>The <a href="https://en.wikipedia.org/wiki/Median">median</a> $\operatorname{Med}X$ is real-valued numbers such that $P(X \leq \operatorname{Med}X) \geq 1/2 \land P(X \geq \operatorname{Med}X) \geq 1/2$. The mode of $X$ is any value $x \in \mathbb{R}$ that maximises $f_X$. Thus, if $P(\vert X \vert &gt; a) &lt; 1/2$ for some $a \in \mathbb{R}^{+}$, then ${\vert \operatorname{Med}X \vert} \leq a$. Also, if $\operatorname{Var}X &lt; \infty$, then ${\vert \operatorname{Med}X - \operatorname{E}X \vert} = {\vert \operatorname{E}(\operatorname{Med}X - X) \vert} \leq \operatorname{E}{\vert X - \operatorname{Med}X \vert} \leq \operatorname{E}{\vert X - \operatorname{E}X \vert} \leq \sqrt{\operatorname{E}{\vert X - (\operatorname{E}X)^2 \vert}}$, where $\sqrt{\operatorname{E}{\vert X - (\operatorname{E}X)^2 \vert}} = \sqrt{\operatorname{Var}X}$. The first and the third inequalities are due to Jensen’s inequality being applied with each convex functions $\varphi(x) = \vert x \vert$ and $\varphi(x) = x^2$.</p>

<p>We say $X$ is <a href="https://www.planetmath.org/symmetricrandomvariable">symmetric around $a$</a> if two shifted random variables $X-a$ and $a-X$ have the same distribution $F_{X-a} = F_{a-X}$ for all $x\in\mathbb{R}$. We also say that $X$ is symmetric around $a$ if $P(X \geq a+x) = P(X \leq a-x)$ for all $x \in \mathbb{R}$, because $F_{X-a} = F_{a-X}$ if and only if $f_{X-a}(a+x) = f_{X-a}(a-x)$. We simply say $X$ is <a href="http://dept.stat.lsa.umich.edu/~moulib/Homework2-fall05sol.pdf">symmetric</a> if $F_X = F_{-X}$ for all $x\in\mathbb{R}$, or, samely, $f_{X}(x) = f_{X}(-x)$ for all $x\in\mathbb{R}$. In fact, if $X$ is symmetric, then we always have a finite $\operatorname{E}X$ such that $\operatorname{E}X = \operatorname{E}(-X) = -\operatorname{E}X$. If, in addition, $X$ is unimodal, then $\operatorname{E}X = \operatorname{Med}X = \operatorname{Mode}X$, or, if $X$ is bimodal, then $\operatorname{E}X = \operatorname{Med}X$. It is often easier to work with symmetric $X$.</p>

<p>We can define a symmetrised random variable $X^s = X - X^\prime$ whose $\operatorname{E}X^s = 0$, by introducing an ind. random variable $X^\prime \stackrel{d}{=} X$, for some non symmetric $X$. That is, for all $a, b \in \mathbb{R}$, the <a href="https://books.google.co.kr/books?id=9TmRgPg-6vgC&amp;pg=PA134&amp;lpg=PA134&amp;dq=strong+symmetrisation+inequalities&amp;source=bl&amp;ots=R5zmAxXEqz&amp;sig=ACfU3U10RulyGkz9LqpvwXoR1jsEu52qvQ&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwjbo8P-rqX2AhUPZ94KHfWaAS4Q6AF6BAgsEAM#v=onepage&amp;q=strong%20symmetrisation%20inequalities&amp;f=false">weak symmetrisation inequalities</a> assure that $P({\vert X - \operatorname{Med}X \vert} \geq b) \leq 2P(X^s \geq b) \leq 4P({\vert X - \operatorname{Med}X \vert} \geq b/2)$. For all $(a_{n})_{n\in\mathbb{N}}$ and $b$ on $\mathbb{R}$, the <a href="">strong symmetrisation inequalities</a> assure $P(\max_{1 \leq k \leq n}{\vert X_k - \operatorname{Med}X_k \vert} \geq b) \leq 2P(\max_{1 \leq k \leq n}{\vert X_k^s \vert} \geq b) \leq 4P(\max_{1 \leq k \leq n}{\vert X_k - \operatorname{Med}X_k \vert} \geq b/2)$. The relation is akin to that of Chebyshev’s and Kolmogorov’s. The median and symmetrisation bounds sums of random variables.</p>

<h2>**</h2>
<hr />
<p><strong>(#1)</strong> <a href="https://en.wikipedia.org/wiki/Bernstein_inequalities_(probability_theory)">Bernstein inequalities</a> can be an alternative, rediscovered in the form of <a href="https://en.wikipedia.org/wiki/Chernoff_bound">Chernoff’s inequality</a>, <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">Hoeffding’s inequality</a> and <a href="https://en.wikipedia.org/wiki/Azuma%27s_inequality">Azuma’s inequality</a> - for the sum of random variables. <strong>(#2)</strong> We define a new r.v. that is asymptotically equiv. and easier to deal with. <strong>(#3)</strong> If we define a distance within a vector space $V$ by $d(X,Y) = {\left\lVert X-Y \right\rVert}_p$, then $d(X,Z) \leq d(X,Y) + d(Y,Z)$ implies that $V$ is a metric space. <strong>(#4)</strong> For any $p,q \in [1, \infty]$, we say $q$ is a <a href="https://math.stackexchange.com/questions/2757548/why-is-the-dual-exponent-what-it-is">dual exponent</a> of $p$ if only if $p^{-1} + q^{-1} = 1$. <strong>(#5)</strong> If $\theta x_1 + (1-\theta)x_2 \in C$ for all $x_1, x_2 \in C$ with $0 \leq \theta \leq 1$, where $C \subseteq \mathbb{R}$, then $C$ is called a <a href="https://en.wikipedia.org/wiki/Convex_set">convex set</a>. <strong>(#6)</strong> Dividing and taking log always helps.</p>

				
					<br>
<hr>
<br>
<div class="comment">
	<p>I gathered words solely for my own purposes without any intention to break the rigorosity of the subjects.<br>
	Well, I prefer eating corn in spiral <i class="fa fa-cutlery"></i>.</p>
</div>
				
			</div>
		</div>
    
	<script src="/assets/js/001.js"></script>
	<script src="/assets/js/002.js"></script>
	<div class="footer">
		<p>Code licensed under <a href="https://github.com/h01000110/h01000110.github.io/blob/master/LICENSE" target="_blank">MIT License</a></p>
	</div>
</body>
</html>