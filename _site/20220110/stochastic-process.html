<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
	<link rel="stylesheet" href="/assets/css/atom-one-light.css">
    
        <title>210. stochastic process</title>
		<link rel="stylesheet" type="text/css" href="/assets/css/002.css">
    
	<link rel="stylesheet" href="/assets/css/font-awesome.min.css">
	<link rel="shortcut icon" href="/assets/img/favicon.ico" type="image/x-icon">
	<link rel="icon" href="/assets/img/favicon.ico" type="image/x-icon">
	<script src="/assets/js/highlight.pack.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>
	
		<!--http://benlansdell.github.io/computing/mathjax/-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    extensions: [
      "tex2jax.js",
      "MathMenu.js",
      "MathZoom.js",
      "AssistiveMML.js",
      "a11y/accessibility-menu.js"
    ],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    jax: ["input/TeX", "output/CommonHTML"],
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
</script>
<!-- You may add the following into MathJax.Hub.Config:
CommonHTML: {
    scale: 85
}-->
<script type="text/javascript" id="MathJax-script" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
	
</head>
<body>
	<div class="wrapper">
		<div class="default_title">
			<img src="/assets/img/mycomputer.png" />
			
				<h1>Hikikomori</h1>
			
		</div>
		<ul class="topbar">
	<a href="/me"><li><u>A</u>bout</li></a>
	<a href="https://www.linkedin.com/in/sung-kim-28b78a7b/" target="_blank"><li><u>L</u>inkedIn</li></a>
	<!-- 
		<a href="http://soundcloud.com/h01000110" target="_blank"><li><u>S</u>oundcloud</li></a>
	-->
</ul>
		<div class="tag_list">
			<ul id="tag-list">
				<li><a href="/" ><img src="/assets/img/disk.png" />(C:)</a>
			<ul>
				
				
				<li><a href="/tag/cs300/" title="cs300"><img src="/assets/img/folder.ico" />cs300</a></li>
				
				<li><a href="/tag/cs400/" title="cs400"><img src="/assets/img/folder.ico" />cs400</a></li>
				
				<li><a href="/tag/math200/" title="math200"><img src="/assets/img/folder.ico" />math200</a></li>
				
				<li><a href="/tag/math500/" title="math500"><img src="/assets/img/folder.ico" />math500</a></li>
				
			</ul>
				</li>
			</ul>
		</div>
		<div class="post_list">
			
				<ul>
					
					<li><a href="/20240201/functional-analysis" title="501. functional analysis"><img src="/assets/img/file.ico" title="501. functional analysis" />501. functional analysis</a></li>
					
					<li><a href="/20240105/parallel-computing" title="Parallel Computing"><img src="/assets/img/file.ico" title="Parallel Computing" />Parallel Computing</a></li>
					
					<li><a href="/20240104/networking" title="Networking"><img src="/assets/img/file.ico" title="Networking" />Networking</a></li>
					
					<li><a href="/20240103/virtualisation" title="Virtualisation"><img src="/assets/img/file.ico" title="Virtualisation" />Virtualisation</a></li>
					
					<li><a href="/20240102/tmp" title="402. operating system"><img src="/assets/img/file.ico" title="402. operating system" />402. operating system</a></li>
					
					<li><a href="/20240102/operating-system" title="402. operating system"><img src="/assets/img/file.ico" title="402. operating system" />402. operating system</a></li>
					
					<li><a href="/20240101/computer" title="401. dl compilation"><img src="/assets/img/file.ico" title="401. dl compilation" />401. dl compilation</a></li>
					
					<li><a href="/20230101/ml-paper" title="301. ml literature"><img src="/assets/img/file.ico" title="301. ml literature" />301. ml literature</a></li>
					
					<li><a href="/20220112/markov-chain" title="212. markov chain"><img src="/assets/img/file.ico" title="212. markov chain" />212. markov chain</a></li>
					
					<li><a href="/20220111/martingale" title="211. martingale"><img src="/assets/img/file.ico" title="211. martingale" />211. martingale</a></li>
					
					<li><a href="/20220110/stochastic-process" title="210. stochastic process"><img src="/assets/img/file.ico" title="210. stochastic process" />210. stochastic process</a></li>
					
					<li><a href="/20220109/laws-of-the-iterated-logarithm" title="209. law of the iterated logarithm"><img src="/assets/img/file.ico" title="209. law of the iterated logarithm" />209. law of the iterated logarithm</a></li>
					
					<li><a href="/20220108/central-limit-theorem" title="208. central limit theorem"><img src="/assets/img/file.ico" title="208. central limit theorem" />208. central limit theorem</a></li>
					
					<li><a href="/20220107/laws-of-large-number" title="207. law of large number"><img src="/assets/img/file.ico" title="207. law of large number" />207. law of large number</a></li>
					
					<li><a href="/20220106/modes-of-convergence" title="206. mode of convergence"><img src="/assets/img/file.ico" title="206. mode of convergence" />206. mode of convergence</a></li>
					
					<li><a href="/20220105/characteristic-function" title="205. characteristic function"><img src="/assets/img/file.ico" title="205. characteristic function" />205. characteristic function</a></li>
					
					<li><a href="/20220104/inequality" title="204. inequality"><img src="/assets/img/file.ico" title="204. inequality" />204. inequality</a></li>
					
					<li><a href="/20220103/expectation" title="203. expectated value"><img src="/assets/img/file.ico" title="203. expectated value" />203. expectated value</a></li>
					
					<li><a href="/20220102/random-variable" title="202. random variable"><img src="/assets/img/file.ico" title="202. random variable" />202. random variable</a></li>
					
					<li><a href="/20220101/probability" title="201. probability theory"><img src="/assets/img/file.ico" title="201. probability theory" />201. probability theory</a></li>
					
				</ul>
			
		</div>
		<div class="post_total">
			
				<div class="left">20 object(s)</div>
			
			<div class="right">&nbsp;</div>
		</div>
	</div>
	
        <div class="content">
			<div class="post_title">
				<img src="/assets/img/file.png" />
				<h1>210. stochastic process</h1>
				<a href="/"><div class="btn"><span class="fa fa-times"></span></div></a>
				<div class="btn btn_max"><span class="fa fa-window-maximize"></span></div>
				<div class="btn"><span class="fa fa-window-minimize"></span></div>
			</div>
			<ul class="topbar">
				<li>January 10, 2022</li>
			</ul>
			<div class="post_content">
        		<h1 id="stochastic-process">Stochastic Process</h1>
<hr />
<p>The next three notes borrow Billingsley’s <a href="">Probability and Measure</a> and Doob’s <a href="">Stochastic Processes</a>. A stochastic process is a vast object that can easily be <a href="https://www.stat.cmu.edu/~cshalizi/754/">sub-grouped</a>. It requires times to comprehend its intricacies, navigate its components, and unveil the underlying patterns or behaviors that govern its evolution over time or across different dimensions.</p>

<h2 id="i">I</h2>
<hr />
<p>Let $T$ be a <a href="https://en.wikipedia.org/wiki/Total_order">totally ordered set</a>, $(\Omega, \mathcal{F}, P)$ be a probability space, and $(\mathbb{S}, \mathcal{S})$ be a <a href="https://en.wikipedia.org/wiki/State_space_(computer_science)">state space</a>. A <a href="https://en.wikipedia.org/wiki/Stochastic_process">stochastic process</a> $X = (X_{t})_{t \in T}$ is a sequence consists of $X_{t}: \Omega \to \mathbb{S}$ such that $X_{t}$ at some $t \in T$ may not be well-defined on $\mathbb{S} \subseteq \mathbb{R}$ <strong>(#1)</strong>. One can index a discrete and a continuous process by $T = \mathbb{N}$ and $T = [0,\infty)$, respectively. Because $X:T\times\Omega\to\mathbb{R}$ is a function of $t \in T$ and $\omega \in \Omega$, we can write $X_{t}$, $X(t)$, $X_{t}(\omega)$, or $X(t,\omega)$, interchangeably. In addition, a function $X(\,\cdot, \omega): T \to \mathbb{R}$ with any $\omega \in \Omega$ is called a <a href="https://math.stackexchange.com/questions/1472068/what-is-a-sample-path-of-a-stochastic-process">sample path</a> or realisation of $X$, and a $\sigma$-algebra $\sigma(\mathbb{R})$ over $T$ which often denotes times contains full of pdfs and/or pmfs.</p>

<p>A full information about $X$ is accessible from the $\sigma$-algebra generated by $\bigcup_{t=1}^{\infty}X_t$, assumed that $X$ lies over the infinite time. If $X$ is on a finite time interval $T = \lbrace 1, 2, \dots, n \rbrace$, then useful information relevant to $X$ is conveyed by the $\sigma$-algebra generated by the set $\bigcap_{t=1}^{n}\lbrace \omega: X_{t}(\omega) \leq x_{t} \rbrace$. In particular, for all $x = (x_1, x_2, \dots, x_n) \in \mathbb{R}^n$, the joint cdf of $X$ is given by $F_X(x) = \mathbb{P}((-\infty, x])$ and a collection of $F_{X_t}(\cdot)$ is called a <a href="https://en.wikipedia.org/wiki/Finite-dimensional_distribution">finite-dimensional distribution</a> (f.d.d.) of $X$. This can be interpreted as the cdf of a random vector $\boldsymbol{X} = (X_{1}, \dots, X_{n}) \in \mathbb{R}^n$ which corresponds to a product measure whenever the selected timestamps yield ind. random variables.</p>

<p>// https://www.stat.cmu.edu/~cshalizi/754/notes/lecture-02.pdf // A marginal dist. of $X_t$ can be relevant to its neighbourhood marginals, and thus we should compute a joint dist.. What causes a problem is that their relationships can change along with $t$.</p>

<h2 id="ii">II</h2>
<hr />
<p>If a f.d.d. of $X$ is time-shift invariant $F_{(X_{1}, \cdots, X_{n})}(x_1, \dots, x_n) = F_{(X_{1+\tau}, \cdots, X_{n+\tau})}(x_{1+\tau}, \dots, x_{n+\tau})$ for all $n \in \mathbb{N}$, then $X$ is said to be <a href="https://en.wikipedia.org/wiki/Stationary_process">strict-sense stationary</a> (sss). If $n \in \lbrace 1, 2, \cdots, N \rbrace$, then $X$ is <a href="">$N$-th order stationary</a> <strong>(#2)</strong>. Whereas, given the 1st moment and the 2nd central moment of $X$ are defined by $\mu_X(t) = \operatorname{E}X_t$ and $\gamma_{XX}(t,s) = \operatorname{Cov}(X_{t}, X_{s})$ for all $t,s \in T$, respectively, if $\mu_X(t) = \mu_X$ and $\gamma_{XX}(t, s) = \gamma_{XX}(\tau, 0)$ for $s \leq t$ and $\tau = t - s$, then $X$ is <a href="">weak-sense stationary</a> (wss). Note that an <a href="https://en.wikipedia.org/wiki/Autocovariance">autocovariance</a> yields an <a href="https://en.wikipedia.org/wiki/Autocorrelation">autocorrelation</a> $\rho_{XX}(t,s) = \gamma_{XX}(t,s)[\sigma_{X}(t)\sigma_{X}(s)]^{-1}$, while we much prefer working with the wss as the sss is highly restrictive.</p>

<p>Let $X$ be stationary, $\bar{x} = {1\over{2T}}\int_{-T}^{T} X(t)\, \mathrm{d}t$ be the <a href="">time average</a>, and $\mu_{X} = \operatorname{E}X(t) = \int_{-\infty}^{\infty} zf_{X_t}(z)\,\mathrm{d}z$ be the <a href="">ensemble average</a>. If it is true that $\lim_{T\to\infty}\bar{x} = \mu_X$, then it is called <a href="https://www.youtube.com/watch?v=k6y2kzayV6A#t=1520">mean-ergodic</a>, and we can evaluate the statistical properties of $X$ by only using a single long sample path. (i.e. observing all possible samples at some fixed time $t$ may be difficult or even impossible). On the other hand, let $\bar{\gamma}_{XX}(\tau) = {1\over{2T}}\int_{-T}^{T}\,(X(t+\tau)-\mu_X)(X(t)-\mu_X) \, \mathrm{d}t$ be the <a href="">time average estimate</a>, and if $\lim_{T\to\infty}\bar{\gamma}_{XX}(\tau) = \gamma_{XX}(\tau)$, then the object is <a href="">autocovariance-ergodic</a>. A process that is both mean-ergodic and autocovariance-ergodic is <a href="https://en.wikipedia.org/wiki/Ergodic_process">wide-sense ergodic</a> <strong>(#3)</strong>.</p>

<p>For all $s, t \in T$ with $s \leq t$, a stochastic process $X$ has <a href="">independent increments</a> (i.e. differences) if $X_t - X_s$ is independent of $\mathcal{F}_s$, and <a href="https://en.wikipedia.org/wiki/Stationary_increments">stationary increments</a> if $F_{X_t - X_s} = F_{X_{t-s} - X_0}$. Moreover, $X$ has independent and stationary increments if and only if $X$ is a partial sum process consists of a sequence $U = (U_{t})_{t \in T}$ of i.i.d. random variables such that $X_n = \Sigma_{t=1}^{n}U_{t}$. A pdf (f.d.d.) of such process $X$ on $\mathbb{R}^n$ is given by $f_{X}(x) = f_{X_1}(x_{1})f_{X_2-X_1}(x_{2}-x_{1}) \dots f_{X_n - X_{n-1}}(x_{n}-x_{n-1})$ when a dist. of $X_{t}$ on $\mathbb{R}$ is known for all $t \in T$. In particular, there exists $\mu \in \mathbb{R}$ and $\sigma \in [0,\infty)$ such that $\mu_{X}(t) = \mu{t}$ and $\rho_{XX}(t,s) = \sigma^{2}\min(s,t)$ for all $s,t \in T$, if $X \in L^2$ <strong>(#4)</strong>.</p>

<h2 id="iii">III</h2>
<hr />
<p>A <a href="https://en.wikipedia.org/wiki/Random_walk">random walk</a> on $\mathbb{Z}$, introduced by Karl Pearson in 1905, is a fundamental example of a stationary stochastic process. Specifically, a partial sum process $(S_{t})_{t \in T}$, consists of $S_0 = 0$ and $S_t = \Sigma_{k=1}^{t}X_k$ with ind. random variables $X_{k} \sim \operatorname{U}(-1, 1)$, is also known as a simple random walk of $\operatorname{E}S_t = 0$ and $\operatorname{Var}S_{t} = t$ <strong>(#5)</strong>. More generally, suppose $(I_{t})_{t \in T}$ consists of a Bernoulli trial $I_{t} = (X_{t} + 1)/2$ of a success probability $p$, then $R_{t} = \Sigma_{k=1}^{t} I_{k} \sim \operatorname{Binom}(t,p)$. In particular, if $X_{t} = 1$ with $p$ and $X_{t} = -1$ with $q=1-p$, then $\operatorname{E}X_{t} = 2p-1$ and $\operatorname{Var}X_{t} = 4pq$, thus $p$ determines a random variable $Y_{t} = 2R_{t}-t$ by $\operatorname{E}Y_{t} = t(2p-1)$ and $\operatorname{Var}Y_{t} = 4tpq$ <strong>(#6)</strong>.</p>

<p>A <a href="">Poisson process</a> consists of i.i.d. $X_t \sim \operatorname{Exp}(\lambda)$ is another 2nd order process of stationary and ind. increments. The process has $\operatorname{E}X_{t} = \operatorname{Var}X_{t} = \lambda{t}$ for all $t \in [0,\infty)$, so does monotonically increasing sample paths; Remark. given a set of $\mathbb{C}$-valued random variables $X_t \in L^2$ such that $\operatorname{E}X_t = 0$, if the inner product in the $L^2$-space is well-defined by $\langle X_{t_1}, X_{t_2} \rangle = \operatorname{E}[X_{t_1}\overline{X}_{t_2}]$ (and thus, ${\left\lVert X_t \right\rVert}_{2} = \sqrt{\langle X_t, X_t \rangle}$) for all $t \in T$, then $(X_t)_{t \in T} \subseteq L^2$ constructs a Hilbert space $H$. If the index set $T$ contains non-negative real numbers, in turn, $(X_t)_{t \in T}$ can be regarded as a curve $C$ exsiting in $H$;</p>

<p>OLS can also be studied in <a href="https://en.wikipedia.org/wiki/Time_series">time series</a> literature if any stochastic process (i.e. panel data) is stationary and under the assumptions of linear regression (taken for cross sectional data) <strong>(#7)</strong>. Non-stationarities are usually hidden under the trend, seasonality, and/or heteroskedasticity. One can use the <a href="">log-differencing</a> to stablise the mean, and/ use the <a href="https://www.youtube.com/watch?v=DeORzP0go5I">acf / pacf plots</a> to observe autocorrelations and stablise the variacne of a time series. Refer to <a href="https://otexts.com/fpp2/stl.html">STL decomposition</a> $y_{t} = S_{t} + T_{t} + R_{t}$ (or $\log{y_{t}} = \log{S_{t}} + \log{T_{t}} + \log{R_{t}}$), where $S_{t}$, $T_{t}$, $R_{t}$ are the seasonal, trend, and remainder parts, respectively.</p>

<h2 id="iv">IV</h2>
<hr />
<p>Let $(\Omega, \mathcal{F}, P)$ be a probability space, $X = (X_t)_{t \in T}$ be a stochastic process, and $F = (\mathcal{F}_t)_{t \in T}$ be a <a href="">filtration</a> representing a family of sub-$\sigma$-algebras $\mathcal{F}_{0} \subseteq \mathcal{F}_1 \subseteq \dots \subseteq \mathcal{F}_{s} \subseteq \mathcal{F}_{t} \subseteq \dots \subseteq \mathcal{F}$. We say $X$ is <a href="">adapted</a> to the filtration $F$ if $X_{t}:\Omega \to \mathbb{R}$ is $\mathcal{F}_{t}$-measurable, and <a href="">predictable</a> by $F$ if $X_{t+1}$ is $\mathcal{F}_{t}$-measurable, for all $t \in T$. A <a href="">natural filtration</a> $\mathcal{F}_{t} = \sigma(X_{s} \,\vert\, s \leq t)$ is often used as the smallest filtration containing all information up to time $t$. In addition, given that both $F$ and $G$ are on any fixed sample space $(\Omega, \mathcal{F})$, then we say $F$ is <a href="https://math.stackexchange.com/questions/4769127/what-does-coarse-mean-in-the-context-of-probability-bounds-and-markovs-inequa">coarser</a> than $G$ and $G$ is finer than $F$, denoted by $F \preceq G$, if $F_t \subseteq G_t$ for all $t \in T$.</p>

<p>We call a random variable $\tau: \Omega \to T$ on a <a href="">filtered probability space</a> $(\Omega, \mathcal{F}, F, P)$ a <a href="">stopping time</a> with respect to $F$ if $\lbrace \tau \leq t \rbrace \in \mathcal{F}_t$ for all $t \in T$, and all information stacked up to $\tau$ is encoded by a <a href="">stopped $\sigma$-algebra</a> $\mathcal{F}_{\tau} = \lbrace A \in \mathcal{F}_{\infty}: A \cap \lbrace \tau \leq t \rbrace \in \mathcal{F}_t, \,\forall t \in T \rbrace$, where an event $A \in S$. $\tau$ is a stopping time if and only if $\lbrace \tau &gt; t \rbrace \in \mathcal{F}_t$ for all $t \in T$. If an arbitrary $\tau \in T$ is a stopping time w.r.t. $F$, and $F$ is coarser than $G$, then $\tau$ is a stopping time with respect to $G$. Moreover, if $\tau_1, \tau_2 \in F$ are stopping times w.r.t. $F$, then (i) $\tau_1 \vee \tau_2 = \max \lbrace \tau_1, \tau_2 \rbrace$; (ii) $\tau_1 \wedge \tau_2 = \min \lbrace \tau_1, \tau_2 \rbrace$; (iii) $\tau_1 + \tau_2$; are also stopping times w.r.t. $F$.</p>

<p>Loosely speaking, $\tau$ decides whether or not to stop evolving $X$ at time $t$ based only on information up to $t$ (i.e. no future information). That is, if $\mathcal{F}_t = F$ for all $t \in T$, then every $t$ are a stopping time. Similarly, if $\mathcal{F}_t = \lbrace \Omega, \emptyset \rbrace$ for all $t \in T$, then $\tau(\omega) = t$ for all $\omega \in \Omega$ with some $t \in T_{\infty}$ <strong>(#8)</strong>. Whilst the examples contain impracticality, a <a href="">first hitting time</a> $\tau_B = \inf \lbrace t: X_t \in B \rbrace$ is a useful stopping time which $\lbrace \tau_B = t \rbrace = \lbrace X_0 \notin B, \dots, X_s \notin B, X_t \in B \rbrace = \bigcap_{r=0}^{s} {\lbrace X_r \in B \rbrace}^c \cap \lbrace X_t \in B \rbrace \in \mathcal{F}_{t}$. Whereas, $\tau_{B}^{\prime} = \sup \lbrace t: X_t \in B \rbrace$ is not a stopping time since a full trajectory of $X$ throughout $T$ must be given in order to have a stopping strategy.</p>

<h2>**</h2>
<hr />
<p><strong>(#1)</strong> Let $D = \text{discrete}, C = \text{continuous}, T = \text{time}, S = \text{space}$, so that we get $(DT,DS), (DT,CS), (CT,DS), (CT,CS)$. <strong>(#2)</strong> If $X$ is $N$-th order stationary, then it is $M$-th order stationary, where $M&lt;N$. <strong>(#3)</strong> Ergodicity is a subset of wss and hence sss. <strong>(#8)</strong> I.e. $\tau$ is a constant. <strong>(#4)</strong> It is true because means and variances of $X$ satisfy the <a href="">Cauchy’s functional equation</a>. <strong>(#5)</strong> White noise is an example of continuous sequnece of i.i.d. random variables. <strong>(#6)</strong> If a random walk is permitted to continue walking forever on $\mathbb{Z}$, it will cross every point an infinite number of times. <strong>(#7)</strong> Otherwise, the model will capture non-stationarity within the processes, turning it into a <a href="https://en.wikipedia.org/wiki/Spurious_relationship">spurious regression</a>.</p>

<p>http://www.scieng.net/tech/12904?page=96</p>

				
					<br>
<hr>
<br>
<div class="comment">
	<p>I gathered words solely for my own purposes without any intention to break the rigorosity of the subjects.<br>
	Well, I prefer eating corn in spiral <i class="fa fa-cutlery"></i>.</p>
</div>
				
			</div>
		</div>
    
	<script src="/assets/js/001.js"></script>
	<script src="/assets/js/002.js"></script>
	<div class="footer">
		<p>Code licensed under <a href="https://github.com/h01000110/h01000110.github.io/blob/master/LICENSE" target="_blank">MIT License</a></p>
	</div>
</body>
</html>