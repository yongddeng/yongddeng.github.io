---
layout: default
title: "301. ml literature"
tags: cs300
use_math: true
---


## I. List of Conferences
---
More will be added if necessary.
$\\\\$

| **Conference**                                                               | **Alias**      | **Founded**    |
| :--------------------------------------------------------------------------- | :------------- | :------------: |
| Association for the Advancement of Artificial Intelligence                   | [AAAI]()       | 1980           |
| Association for Computational Linguistics                                    | [ACL]()        | 2013           |
| IEEE Conference on Computer Vision and Pattern Recognition                   | [CVPR]()       | 1983           |
| Conference on Empirical Methods in Natural Language Processing               | [EMNLP]()      | 1987           |
| International Conference on Learning Representations                         | [ICLR]()       | 1962           |
| International Conference on Machine Learning                                 | [ICML]()       | 1996           |
| Institute of Electrical and Electronics Engineers                            | [IEEE]()       | 1963           |
| International Joint Conference on Artificial Intelligence                    | [IJCAI]()      | 1969           |
| Conference and Workshop on Neural Information Processing Systems             | [NeurIPS]()    | 1982           |
| ACM Conference on Recommender Systems                                        | [RecSys]()     | 2007           |

$\\\\$


## II. List of Papers
---
Those should be organised better.


**_arXiv**

    Harnessing the Universal Geometry of Embeddings, 2025
    "DeepSeek-R1": Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, 2025
    Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling, 2025
    Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters, 2024
    "Mixtral": Mixtral of Experts (8x7B), 2024
    SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills, 2023
    A Survey of Large Language Models, 2023
    Efficient Memory Management for Large Language Model Serving with PagedAttention, 2023
    "Mamba": Linear-Time Sequence Modeling with Selective State Spaces, 2023
    "LLaMA": Open and Efficient Foundation Language Models, 2023
    "Chinchilla": Training Compute-Optimal Large Language Models, 2022
    FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness, 2022
    LoRA: Low-Rank Adaptation of Large Language Models, 2021
    "Dall-E": Zero-Shot Text-to-Image Generation, 2021
    "T5": Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, 2019
    "RoBERTa": A Robustly Optimized BERT Pretraining Approach, 2019
    "GPT-2": Language Models are Unsupervised Multitask Learners, 2018
    Representation Learning with Contrastive Predictive Coding, 2018
    Deep Learning based Recommender System: A Survey and New Perspectives, 2017


**AAAI**

    Vision Transformers are Robust Learners, 2022
    Multi-Task Learning of Knowledge Tracing and Option Tracing for Better Student Assessment, 2022


**ACL**

    Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks, 2021
    "BART": Denoising Seq2Seq Pre-training for NLG, Translation, and Comprehension, 2020
    Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned, 2019
    What Does BERT Look At? An Analysis of BERT`s Attention, 2019
    Self-Attention with Relative Position Representations, 2018
    Universal Language Model Fine-tuning for Text Classification, 2018
    "Transformer-XL": Attentive Language Models Beyond a Fixed-Length Context, 2018
    "BERT": Pre-training of Deep Bidirectional Transformers for Language Understanding, 2018
    Improving Neural Network Translation Models with Multiple Subword Candidates, 2018
    Neural Machine Translation of Rare Words with Subword Units, 2016


**CVPR**

    "CLIP": Learning Transferable Visual Models From Natural Language Supervision, 2021
    FaceNet: A Unified Embedding for Face Recognition and Clustering, 2015


**EMNLP**

    Text Embeddings Reveal (Almost) As Much As Text, 2023
    GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints, 2023
    "SimCSE": Simple Contrastive Learning of Sentence Embeddings, 2021
    Dense Passage Retrieval for Open-Domain Question Answering, 2020
    BERT-flow: On the Sentence Embeddings from Pre-trained Language Models, 2020
    "Sentence-BERT": Sentence Embeddings using Siamese BERT-Networks, 2019
    How Contextual are Contextualized Word Representations?, 2019
    Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks, 2019
    A simple and language independent subword tokenizer and detokenizer for Neural Text Processing, 2018
    Effective Approaches to Attention-based Neural Machine Translation, 2015
        

**ICLR**

    Demystifying Embedding Spaces using Large Language Models, 2024
    "FLAN": Finetuned Language Models are Zero-Shot Learners, 2022
    "BEiT": Bert Pre-Training of Image Transfomers, 2022
    "ViT": An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, 20204. 
    "Performer": Rethinking Attention with Performers, 2020
    Decoupled Weight Decay Regularization, 2019
    Don`t Decay the Learning Rate, Increase the Batch Size, 2018
    On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima, 2017
    Neural Machine Translation by Jointly Learning to Align and Translate, 2015


**ICML**

    Fast Inference from Transformers via Speculative Decoding, 2023
    "SimCLR": A Simple Framework for Contrastive Learning of Visual Representations, 2020
    Understanding Contrastive Learning through Alignment and Uniformity on the Hypersphere, 2020
    On Layer Normalization in the Transformer Architecture, 2020
    Towards A Unified Analysis of Random Fourier Features, 2019


**IEEE**

    "LiT": Zero-Shot Transfer with Locked-image text Tuning, 2022
    "Swin Transformer": Hierarchical Vision Transformer using Shifted Windows, 2021
    Random Features for Kernel Approximation: A survey on Algorithms, Theory, and Beyond, 2020
    "Faiss": Billion-scale similarity search with GPUs, 2017
    Efficient and robust approximate nearest neighbor search using HNSW graphs, 2016
    Optimized Product Quantization, 2013
    Product Quantization for Nearest Neighbor Search, 2011
    Factorization Machines, 2010
    Collaborative Filtering for Implicit Feedback Datasets, 2008


**IJCAI**

    "DeepFM": A Factorization-Machine based Neural Network for CTR Prediction, 2016


**NeurIPS**

    The Impact of Positional Encoding on Length Generalization in Transformers, 2023
    QLoRA: Efficient Finetuning of Quantized LLMs, 2023
    LLM.int8: 8-bit Matrix Multiplication for Transformers at Scale, 2022
    Training language models to follow instructions with human feedback, 2022
    Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, 2022
    "RAG": Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, 2020
    "GPT-3": Language Models are Few-Shot Learners, 2020
    "XLNet": Generalized Autoregressive Pretraining for Language Understanding, 2019
    When does Label Smoothing Help?, 2019
    Visualizing the Loss Landscape of Neural Nets, 2018
    How Does Batch Normalization Help Optimization?, 2018
    Train longer, generalize better: closing the generalization gap in large batch training of NNs, 2017
    "Transformer": Attention is All you Need, 2017


**RecSys**

    Logistic Matrix Factorization for Implicit Feedback Data, 2020
    Deep Neural Networks for YouTube Recommendations, 2016
    Wide & Deep Learning for Recommender System, 2016
    Bayesian Personalized Ranking from Implicit Feedback, 2012
    The YouTube video recommendation system, 2010
    Matrix Factorization Technique for Recommender Systems, 2009