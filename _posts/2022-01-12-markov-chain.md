---
layout: default
title: "212. markov chain"
tags: math200
use_math: true
---


# Markov Chain
---
The Markov property formally describes that the future is independent to the past. Weather forecast is an example which depends primarily on the current weather instead of the entire record of history. The [link](https://www.youtube.com/watch?v=si76S7QqxTU&list=PLivJwLo9VCULQQkfmXK_ZXzPYc3JJ0Cpn&index=1) provides many sampling algorithms built upon the Monte Carlo methods and the Markov property.


## I
---
We call a stochastic process $X = (X\_t)\_{t \in T}$ consists of $\mathbb{S}$-valued random variables $X\_{t}:\Omega \to \mathbb{S}$ a [Markov process]() if it satisfies the [Markov property](https://en.wikipedia.org/wiki/Markov_property) $P(X\_{t} = x\_{t} \,\vert\, X\_{t-1} = x\_{t-1}, \dots, X\_{1} = x\_1) = P(X\_{t} = x\_{t} \,\vert\, X\_{t-1} = x\_{t-1})$ for all $t \in T$. We primarily work with a discrete-time Markov process on a countable state space $\mathbb{S}$, called a [Markov chain](https://en.wikipedia.org/wiki/Markov_chain), and so a joint pmf for $t$ consecutive realisations is given by $P(X\_{t}, X\_{t-1}, \dots, X\_{1}) = P(X\_{t}\,\vert\,X\_{t-1}) P(X\_{t-1}\,\vert\,X\_{t-2}) \dots P(X\_{1}) = \prod\_{s=1}^{t-1}P(X\_{s+1}\,\vert\,X\_{s}) P(X\_{1})$. We focus on the initial [state probabilities]() $\pi\_{j}(1) = P(X\_{1} = j)$ and the $1$-step [transition probabilities]() $p\_{ij}(s) = P(X\_{s+1} = j \,\vert\, X\_{s} = i)$, where $i, j \in \mathbb{S}$ **(#1)**.

If $\mathbb{S}$ contains $n$ distinct elements (i.e. finite), then we can write a [transition matrix](https://en.wikipedia.org/wiki/Stochastic_matrix) $\mathbf{P} = (p\_{ij})$ with $\Sigma\_{j \in \mathbb{S}}p\_{ij} = 1$, where $\mathbf{P}$ is an $n \times n$ positive semi-definite matrix. If, in addition, it is [time-homogeneous]() $P(X\_{t} = j \,\vert\, X\_{t-1} = i) = P(X\_{t-1} = j \,\vert\, X\_{t-2} = i)$ for all $t > 2$, then the [Chapman-Kolmogorov equation](https://en.wikipedia.org/wiki/Chapman%E2%80%93Kolmogorov_equation) yields $\mathbf{P}(k)\mathbf{P}(s) = \mathbf{P}(k+s) = [p\_{ij}(k+s)]$. That is, the $k$-step transition matrix is equal to that of $k$-th power $\mathbf{P}^k = \mathbf{P}(k) = [p^{(k)}\_{ij}(1)]$ **(#2)**. For instance, let $t=3$ and $h \in \mathbb{S}$ be an intermediate state in between $i$ and $j$, so we need $p^{(2)}\_{ij}(1) = P(X\_{3} = j \,\vert\, X\_{1} = i) = \Sigma\_{h}P(X\_{3}=j \,\vert\, X\_{2}=h) P(X\_{2}=h \,\vert\, X\_{1}=i) = \Sigma\_{h}p\_{ih}(1)p\_{hj}(1)$ for the joint pmf.

We now consider a state pmf $\pi(t) = [\pi\_{j}(t)]$, where $\pi\_{j}(t) = \Sigma\_{i}p\_{ij}\pi\_{i}(t-1)$. If $\pi\_{j}(t)\to\pi\_{j}$ and $\pi\_{i}(t-1)\to\pi\_{i}$ as $t\to\infty$, then $\lim\_{t\to\infty}\pi(t) = \pi$ is a stationary state pmf or a [steady state](https://www.youtube.com/watch?v=4sXiCxZDrTU), where $\pi$ is a $(1 \times n)$ row vector with $\Sigma\_{j}\pi\_{j} = 1$ and $\pi\_{j} = \Sigma\_{i}p\_{ij}\pi\_{i}$. In fact, the [Perron-Frobenius theorem]() enumerates the eigenvalues of $\mathbf{P}$ by $1= \vert \lambda\_{1} \vert > \vert \lambda\_{2} \vert \geq\cdots\geq \vert \lambda\_{n} \vert$, and also its matrix notation $\pi\mathbf{P}=\pi$ is in the form $v^{\top}A = v^{\top}\lambda$. Thus, the left eigenvector $q\_{1}$ of $q\_{1}^{\top}\mathbf{P} = q^{\top}\_{1}\lambda\_{1}$ is a steady state, and the rate of convergence of $\pi(t)$ is in the order of $\lambda_2/\lambda_1$. A limiting distribution $\pi(1)\mathbf{P}^{\infty}=\pi$ of a time-homogeneous Markov chain, if exists, is always unique.


## II
---
We introduce a few jargons to read the formal statements describing behaviours of a Markov chain and the conditions under which a state pmf becomes steady. A state $j$ is [accessible]() from a state $i$, denoted by $i \rightarrow j$, if $p\_{ij}(s) > 0$ for some $s\geq{1}$. Whereas, $i$ is [absorbing]() if $p\_{ij}(s) = 0$ for some $s\geq{1}$ and all $j \in S$. If both $i \rightarrow j$ and $j \rightarrow i$, and thus $i \leftrightarrow j$, we say $i$ and $j$ [communicate]() with each other **(#3)** and belong to the same [class](). Intuitively, a Markov chain will be consisting of one or more disjoint communication classes, and a system with a single class is said to be [irreducible]() in which all states in the chain communicate with each other.

Suppose $f\_{i} = P(X\_{k} = i \;\text{for some}\; k>1 \,\vert\, X\_{1}=i)$ is a probability that a Markov chain departed from $i$ returns to $i$ at least once, then $i$ is [recurrent]() if $f\_{i} = 1$, and [transient]() if $f\_{i} < 1$. That is, a recurrent state occurs infinitly often, and a transient state occurs finitly often. We can easily assume that a probability of escaping a transient state $i$ follows $\operatorname{Bernoulli}(1-f\_{i})$, and a number of returns to $i$ within a $k$-steps follows $\text{Geom}(1-f\_{i})$. Let $p^{(k)}\_{ii}$ be a probability that a Markov chain departed from $i$ returns to $i$ after $k$-step, then $i$ is recurrent if and only if $\Sigma\_{k=2}^{\infty}p^{(k)}\_{ii} = \infty$, and transient if and only if $\Sigma\_{k=2}^{\infty}p^{(k)}\_{ii} < \infty$, where $p^{(k)}\_{ii}$ is ind. of an index $s$ on a conditioning.

Let $t\_{ii}$ be an elapsed time of returning to $i$ from $i$ such that $\mu\_{ii} = \operatorname{E}t\_{ii}$ is a [mean recurrence time](). We call $i$ a [positive recurrent]() if $\mu\_{ii} < \infty$, and [null recurrent]() if $\mu\_{ii} = \infty$. If, in addition, $i$ is the initial departure and $\lbrace \tau\_{ii}(s) \rbrace\_{1 < s \leq k}$ is a set of i.i.d. elapsed times, then a proportion of time spent in $i$ until $s$-th returns will by $s/\Sigma\_{l=1}^{s}\tau\_{ii}(l)$ such that $s/\Sigma\_{l=1}^{s}\tau\_{ii}(l) \to 1/\mu\_{ii} = \pi\_i$ as $k\to\infty$ by the WLLN. A state $i$ has [period]() $d = \gcd \lbrace k>1:p^{(k)}\_{ii}>0 \rbrace$ **(#4)**. We say $i$ is [periodic]() if $d>1$, and [aperiodic]() if $d=1$. All state in a class share the same $d$ as periodicity, recurrence, and transience are a class property. An irreducible Markov of $d=1$ is aperiodic. 


## III
---
A state $i$ is [ergodic](https://stats.libretexts.org/Bookshelves/Probability_Theory/Book%3A_Introductory_Probability_(Grinstead_and_Snell)/11%3A_Markov_Chains/11.03%3A_Ergodic_Markov_Chains) if it is aperiodic and positive recurrent. An irreducible Markov chain is ergodic if the states are ergodic. Moreover, an ergodic system comes if there exists $N$ such that any state $i$ is reachable from any other state $j$ within any step $n \leq N$. For example, a fully connected transition matrix (i.e. $p\_{ij} > 0$ for all $i,j\in\mathbb{S}$) fulfils the sufficient condition with $N = 1$. A system with more than one state and just one out-going transition per state cannot be ergodic as it is either not irreducible or not aperiodic. Ergodicity shines because it provides a unique steady state which in general varies with an initial state pmf of a regular system.

A Markov chain is [reversible](https://cims.nyu.edu/~holmes/teaching/asa19/handout_Lecture3_2019.pdf) with respect to $\pi$ if it holds the [detailed balanced condition](https://en.wikipedia.org/wiki/Detailed_balance) $\pi\_{i}p\_{ij} = \pi_{j}p\_{ji}$ for all $i,j\in\mathbb{S}$. In particular, if $X$ is a general system and there exists $\pi$ such that the condition is held, then $\Sigma_{i}\pi\_{i}p\_{ij} = \Sigma_{i}\pi\_{j}p\_{ji} = \pi\_{j}\Sigma_{i}p\_{ji} = \pi\_{j}$ and so $X$ holds the global balance condition $\pi = \pi{P}$. These yield a new sampling framework [Markov chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) (MCMC) and overcome the limitation of the [rejection sampling](https://www.youtube.com/watch?v=OXDqjdVVePY) **(5)**. The [Metropolis algorithm](https://www.youtube.com/watch?v=yCv2N7wGDCw) is a popular instance which uses symmetric conditional candidate density functions to return (i.e. after a [burn-in period](http://users.stat.umn.edu/~geyer/mcmc/burn.html)) sequences of numbers which we hope can reflect a target density function.

MCMC under the Bayesian settings can generate observations from the posterior (to estimate parameters $\theta$ given data $\mathbf{x}$) **(6)**. MCMC may suffer the curse of dimensionality whereby regions of higher probability tend to stretch and get lost in an increasing volume of space. A well-designed model that can learn all available information from the entire records may be more sophisticaed than a partly conditioned model. We naively assume the Markov property as computing the joint dist. of such model is impractical. The [PageRank](https://en.wikipedia.org/wiki/PageRank) developed by Larry Page and Sergey Brin in 1996 can be understood as a Markov chain.


## **
---
**(#1)** Many results for Markov chains can be extended to chains with an uncountable state space, so-called Harris chains. **(#2)** We may eigen-decompose a trainsion matrix: $\mathbf{P}^k = \mathbf{Q}\mathbf{\Lambda}^k\mathbf{Q}^{-1}$, so that a diagonal opeartion makes a matrix multiplication more efficient. **(#3)** If $i \rightarrow h$ and $h \rightarrow j$, then $i \rightarrow j$. **(#4)** That is, $p^{(k)}\_{ii} = 0$ whenever $k$ % $d \neq 0$ for all $k > d$. **(5)** It merely uses ind. observations. **(6)** [P Marjoram (2003)](https://www.pnas.org/doi/pdf/10.1073/pnas.0306899100) attempted the same without the use of likelihoods.