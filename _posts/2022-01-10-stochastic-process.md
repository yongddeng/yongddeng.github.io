---
layout: default
title: "210. stochastic process"
tags: math200
use_math: true
---


# Stochastic Process
---
The next three notes borrow Billingsley's [Probability and Measure]() and Doob's [Stochastic Processes](). A stochastic process is a vast object that can easily be [sub-grouped](https://www.stat.cmu.edu/~cshalizi/754/). It requires times to comprehend its intricacies, navigate its components, and unveil the underlying patterns or behaviors that govern its evolution over time or across different dimensions.


## I
---
Let $T$ be a [totally ordered set](https://en.wikipedia.org/wiki/Total_order), $(\Omega, \mathcal{F}, P)$ be a probability space, and $(\mathbb{S}, \mathcal{S})$ be a [state space](https://en.wikipedia.org/wiki/State_space_(computer_science)). A [stochastic process](https://en.wikipedia.org/wiki/Stochastic_process) $X = (X\_{t})\_{t \in T}$ is a sequence consists of $X_{t}: \Omega \to \mathbb{S}$ such that $X\_{t}$ at some $t \in T$ may not be well-defined on $\mathbb{S} \subseteq \mathbb{R}$ **(#1)**. One can index a discrete and a continuous process by $T = \mathbb{N}$ and $T = [0,\infty)$, respectively. Because $X:T\times\Omega\to\mathbb{R}$ is a function of $t \in T$ and $\omega \in \Omega$, we can write $X\_{t}$, $X(t)$, $X\_{t}(\omega)$, or $X(t,\omega)$, interchangeably. In addition, a function $X(\,\cdot, \omega): T \to \mathbb{R}$ with any $\omega \in \Omega$ is called a [sample path](https://math.stackexchange.com/questions/1472068/what-is-a-sample-path-of-a-stochastic-process) or realisation of $X$, and a $\sigma$-algebra $\sigma(\mathbb{R})$ over $T$ which often denotes times contains full of pdfs and/or pmfs.

A full information about $X$ is accessible from the $\sigma$-algebra generated by $\bigcup_{t=1}^{\infty}X_t$, assumed that $X$ lies over the infinite time. If $X$ is on a finite time interval $T = \lbrace 1, 2, \dots, n \rbrace$, then useful information relevant to $X$ is conveyed by the $\sigma$-algebra generated by the set $\bigcap\_{t=1}^{n}\lbrace \omega: X\_{t}(\omega) \leq x\_{t} \rbrace$. In particular, for all $x = (x_1, x_2, \dots, x_n) \in \mathbb{R}^n$, the joint cdf of $X$ is given by $F_X(x) = \mathbb{P}((-\infty, x])$ and a collection of $F\_{X\_t}(\cdot)$ is called a [finite-dimensional distribution](https://en.wikipedia.org/wiki/Finite-dimensional_distribution) (f.d.d.) of $X$. This can be interpreted as the cdf of a random vector $\boldsymbol{X} = (X\_{1}, \dots, X\_{n}) \in \mathbb{R}^n$ which corresponds to a product measure whenever the selected timestamps yield ind. random variables.

// https://www.stat.cmu.edu/~cshalizi/754/notes/lecture-02.pdf // A marginal dist. of $X_t$ can be relevant to its neighbourhood marginals, and thus we should compute a joint dist.. What causes a problem is that their relationships can change along with $t$.


## II
---
If a f.d.d. of $X$ is time-shift invariant $F_{(X\_{1}, \cdots, X\_{n})}(x\_1, \dots, x\_n) = F_{(X\_{1+\tau}, \cdots, X\_{n+\tau})}(x\_{1+\tau}, \dots, x\_{n+\tau})$ for all $n \in \mathbb{N}$, then $X$ is said to be [strict-sense stationary](https://en.wikipedia.org/wiki/Stationary_process) (sss). If $n \in \lbrace 1, 2, \cdots, N \rbrace$, then $X$ is [$N$-th order stationary]() **(#2)**. Whereas, given the 1st moment and the 2nd central moment of $X$ are defined by $\mu_X(t) = \operatorname{E}X_t$ and $\gamma_{XX}(t,s) = \operatorname{Cov}(X\_{t}, X\_{s})$ for all $t,s \in T$, respectively, if $\mu_X(t) = \mu_X$ and $\gamma_{XX}(t, s) = \gamma_{XX}(\tau, 0)$ for $s \leq t$ and $\tau = t - s$, then $X$ is [weak-sense stationary]() (wss). Note that an [autocovariance](https://en.wikipedia.org/wiki/Autocovariance) yields an [autocorrelation](https://en.wikipedia.org/wiki/Autocorrelation) $\rho_{XX}(t,s) = \gamma_{XX}(t,s)[\sigma_{X}(t)\sigma_{X}(s)]^{-1}$, while we much prefer working with the wss as the sss is highly restrictive.

Let $X$ be stationary, $\bar{x} = {1\over{2T}}\int_{-T}^{T} X(t)\, \mathrm{d}t$ be the [time average](), and $\mu\_{X} = \operatorname{E}X(t) = \int_{-\infty}^{\infty} zf\_{X\_t}(z)\,\mathrm{d}z$ be the [ensemble average](). If it is true that $\lim\_{T\to\infty}\bar{x} = \mu\_X$, then it is called [mean-ergodic](https://www.youtube.com/watch?v=k6y2kzayV6A#t=1520), and we can evaluate the statistical properties of $X$ by only using a single long sample path. (i.e. observing all possible samples at some fixed time $t$ may be difficult or even impossible). On the other hand, let $\bar{\gamma}\_{XX}(\tau) = {1\over{2T}}\int\_{-T}^{T}\,(X(t+\tau)-\mu_X)(X(t)-\mu_X) \, \mathrm{d}t$ be the [time average estimate](), and if $\lim\_{T\to\infty}\bar{\gamma}\_{XX}(\tau) = \gamma\_{XX}(\tau)$, then the object is [autocovariance-ergodic](). A process that is both mean-ergodic and autocovariance-ergodic is [wide-sense ergodic](https://en.wikipedia.org/wiki/Ergodic_process) **(#3)**.
 
For all $s, t \in T$ with $s \leq t$, a stochastic process $X$ has [independent increments]() (i.e. differences) if $X\_t - X\_s$ is independent of $\mathcal{F}\_s$, and [stationary increments](https://en.wikipedia.org/wiki/Stationary_increments) if $F\_{X\_t - X\_s} = F\_{X\_{t-s} - X\_0}$. Moreover, $X$ has independent and stationary increments if and only if $X$ is a partial sum process consists of a sequence $U = (U\_{t})\_{t \in T}$ of i.i.d. random variables such that $X_n = \Sigma_{t=1}^{n}U\_{t}$. A pdf (f.d.d.) of such process $X$ on $\mathbb{R}^n$ is given by $f\_{X}(x) = f\_{X\_1}(x\_{1})f\_{X\_2-X\_1}(x\_{2}-x\_{1}) \dots f\_{X\_n - X\_{n-1}}(x\_{n}-x\_{n-1})$ when a dist. of $X\_{t}$ on $\mathbb{R}$ is known for all $t \in T$. In particular, there exists $\mu \in \mathbb{R}$ and $\sigma \in [0,\infty)$ such that $\mu\_{X}(t) = \mu{t}$ and $\rho_{XX}(t,s) = \sigma^{2}\min(s,t)$ for all $s,t \in T$, if $X \in L^2$ **(#4)**.


## III
---
A [random walk](https://en.wikipedia.org/wiki/Random_walk) on $\mathbb{Z}$, introduced by Karl Pearson in 1905, is a fundamental example of a stationary stochastic process. Specifically, a partial sum process $(S\_{t})\_{t \in T}$, consists of $S\_0 = 0$ and $S\_t = \Sigma\_{k=1}^{t}X\_k$ with ind. random variables $X\_{k} \sim \operatorname{U}(-1, 1)$, is also known as a simple random walk of $\operatorname{E}S\_t = 0$ and $\operatorname{Var}S\_{t} = t$ **(#5)**. More generally, suppose $(I\_{t})\_{t \in T}$ consists of a Bernoulli trial $I\_{t} = (X\_{t} + 1)/2$ of a success probability $p$, then $R\_{t} = \Sigma\_{k=1}^{t} I\_{k} \sim \operatorname{Binom}(t,p)$. In particular, if $X\_{t} = 1$ with $p$ and $X\_{t} = -1$ with $q=1-p$, then $\operatorname{E}X\_{t} = 2p-1$ and $\operatorname{Var}X\_{t} = 4pq$, thus $p$ determines a random variable $Y\_{t} = 2R\_{t}-t$ by $\operatorname{E}Y\_{t} = t(2p-1)$ and $\operatorname{Var}Y\_{t} = 4tpq$ **(#6)**.

A [Poisson process]() consists of i.i.d. $X_t \sim \operatorname{Exp}(\lambda)$ is another 2nd order process of stationary and ind. increments. The process has $\operatorname{E}X\_{t} = \operatorname{Var}X\_{t} = \lambda{t}$ for all $t \in [0,\infty)$, so does monotonically increasing sample paths; Remark. given a set of $\mathbb{C}$-valued random variables $X\_t \in L^2$ such that $\operatorname{E}X\_t = 0$, if the inner product in the $L^2$-space is well-defined by $\langle X\_{t\_1}, X_{t\_2} \rangle = \operatorname{E}[X\_{t_1}\overline{X}\_{t\_2}]$ (and thus, ${\left\lVert X\_t \right\rVert}\_{2} = \sqrt{\langle X\_t, X\_t \rangle}$) for all $t \in T$, then $(X\_t)_{t \in T} \subseteq L^2$ constructs a Hilbert space $H$. If the index set $T$ contains non-negative real numbers, in turn, $(X\_t)\_{t \in T}$ can be regarded as a curve $C$ exsiting in $H$;

OLS can also be studied in [time series](https://en.wikipedia.org/wiki/Time_series) literature if any stochastic process (i.e. panel data) is stationary and under the assumptions of linear regression (taken for cross sectional data) **(#7)**. Non-stationarities are usually hidden under the trend, seasonality, and/or heteroskedasticity. One can use the [log-differencing]() to stablise the mean, and/ use the [acf / pacf plots](https://www.youtube.com/watch?v=DeORzP0go5I) to observe autocorrelations and stablise the variacne of a time series. Refer to [STL decomposition](https://otexts.com/fpp2/stl.html) $y\_{t} = S\_{t} + T\_{t} + R\_{t}$ (or $\log{y\_{t}} = \log{S\_{t}} + \log{T\_{t}} + \log{R\_{t}}$), where $S\_{t}$, $T\_{t}$, $R\_{t}$ are the seasonal, trend, and remainder parts, respectively.


## IV
---
Let $(\Omega, \mathcal{F}, P)$ be a probability space, $X = (X\_t)\_{t \in T}$ be a stochastic process, and $F = (\mathcal{F}\_t)\_{t \in T}$ be a [filtration]() representing a family of sub-$\sigma$-algebras $\mathcal{F}\_{0} \subseteq \mathcal{F}\_1 \subseteq \dots \subseteq \mathcal{F}\_{s} \subseteq \mathcal{F}\_{t} \subseteq \dots \subseteq \mathcal{F}$. We say $X$ is [adapted]() to the filtration $F$ if $X\_{t}:\Omega \to \mathbb{R}$ is $\mathcal{F}\_{t}$-measurable, and [predictable]() by $F$ if $X\_{t+1}$ is $\mathcal{F}\_{t}$-measurable, for all $t \in T$. A [natural filtration]() $\mathcal{F}\_{t} = \sigma(X\_{s} \,\vert\, s \leq t)$ is often used as the smallest filtration containing all information up to time $t$. In addition, given that both $F$ and $G$ are on any fixed sample space $(\Omega, \mathcal{F})$, then we say $F$ is [coarser](https://math.stackexchange.com/questions/4769127/what-does-coarse-mean-in-the-context-of-probability-bounds-and-markovs-inequa) than $G$ and $G$ is finer than $F$, denoted by $F \preceq G$, if $F\_t \subseteq G\_t$ for all $t \in T$.

We call a random variable $\tau: \Omega \to T$ on a [filtered probability space]() $(\Omega, \mathcal{F}, F, P)$ a [stopping time]() with respect to $F$ if $\lbrace \tau \leq t \rbrace \in \mathcal{F}\_t$ for all $t \in T$, and all information stacked up to $\tau$ is encoded by a [stopped $\sigma$-algebra]() $\mathcal{F}\_{\tau} = \lbrace A \in \mathcal{F}\_{\infty}: A \cap \lbrace \tau \leq t \rbrace \in \mathcal{F}\_t, \,\forall t \in T \rbrace$, where an event $A \in S$. $\tau$ is a stopping time if and only if $\lbrace \tau > t \rbrace \in \mathcal{F}\_t$ for all $t \in T$. If an arbitrary $\tau \in T$ is a stopping time w.r.t. $F$, and $F$ is coarser than $G$, then $\tau$ is a stopping time with respect to $G$. Moreover, if $\tau\_1, \tau\_2 \in F$ are stopping times w.r.t. $F$, then (i) $\tau\_1 \vee \tau\_2 = \max \lbrace \tau\_1, \tau\_2 \rbrace$; (ii) $\tau\_1 \wedge \tau\_2 = \min \lbrace \tau\_1, \tau\_2 \rbrace$; (iii) $\tau\_1 + \tau\_2$; are also stopping times w.r.t. $F$.

Loosely speaking, $\tau$ decides whether or not to stop evolving $X$ at time $t$ based only on information up to $t$ (i.e. no future information). That is, if $\mathcal{F}\_t = F$ for all $t \in T$, then every $t$ are a stopping time. Similarly, if $\mathcal{F}\_t = \lbrace \Omega, \emptyset \rbrace$ for all $t \in T$, then $\tau(\omega) = t$ for all $\omega \in \Omega$ with some $t \in T\_{\infty}$ **(#8)**. Whilst the examples contain impracticality, a [first hitting time]() $\tau\_B = \inf \lbrace t: X\_t \in B \rbrace$ is a useful stopping time which $\lbrace \tau\_B = t \rbrace = \lbrace X\_0 \notin B, \dots, X\_s \notin B, X\_t \in B \rbrace = \bigcap\_{r=0}^{s} {\lbrace X\_r \in B \rbrace}^c \cap \lbrace X\_t \in B \rbrace \in \mathcal{F}\_{t}$. Whereas, $\tau\_{B}^{\prime} = \sup \lbrace t: X\_t \in B \rbrace$ is not a stopping time since a full trajectory of $X$ throughout $T$ must be given in order to have a stopping strategy.


## **
---
**(#1)** Let $D = \text{discrete}, C = \text{continuous}, T = \text{time}, S = \text{space}$, so that we get $(DT,DS), (DT,CS), (CT,DS), (CT,CS)$. **(#2)** If $X$ is $N$-th order stationary, then it is $M$-th order stationary, where $M<N$. **(#3)** Ergodicity is a subset of wss and hence sss. **(#8)** I.e. $\tau$ is a constant. **(#4)** It is true because means and variances of $X$ satisfy the [Cauchy's functional equation](). **(#5)** White noise is an example of continuous sequnece of i.i.d. random variables. **(#6)** If a random walk is permitted to continue walking forever on $\mathbb{Z}$, it will cross every point an infinite number of times. **(#7)** Otherwise, the model will capture non-stationarity within the processes, turning it into a [spurious regression](https://en.wikipedia.org/wiki/Spurious_relationship).



http://www.scieng.net/tech/12904?page=96